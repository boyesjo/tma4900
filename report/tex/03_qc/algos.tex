\section{Quantum algorithms}

\subsection{Grover's algorithm}
The quantum search algorithm of Grover~\autocite{grover1996} is a quantum algorithm that finds an element in an unstructured list with high probability.
While such a problem necessarily requires $O(N)$ time in a classical setting, needing on average $N/2$ steps to find the element and in the worst case $N$, Grover's algorithm finds the element in $O(\sqrt{N})$ steps.
This is a quadratic speed-up.

Grover's algorithm is provably optimal; no quantum algorithm can perform such general searches faster~\autocite{zalka1999}.
This should not be surprising.
If an exponential speed-up were possible, Grover search could be used to find the solution to NP-hard problems fast.

For Grover's algorithm to work, assume there is a function $f$ that maps the index of an element to $1$ if it is the one desired and $0$ otherwise.
Then, one assumes access to a quantum oracle, $\mathcal{O}_f$
(effectively a black box subroutine) that implements $f$ thus:
\begin{equation}
    \label{eq:grover_oracle}
    \mathcal{O}_f \ket{x} = (-1)^{f(x)} \ket{x}.
\end{equation}
A single application of this oracle is not enough to find the desired element, as the square of the amplitude of the desired element remains unchanged.
Central to Grover's algorithm is the idea of amplifying the amplitude of the desired element.
This is done by applying a sequence of operations that is repeated until the amplitude of the desired element is large enough for it is most likely to be measured, while the amplitudes of the other elements are reduced.

Let the state $\ket{w}$ which be the winner state, a state with amplitude $1$ for the desired element and $0$ for all others.
Then consider the state $\ket{s}$, which is a uniform superposition state, a state with equal amplitudes for all elements.
Define the state $\ket{s'}$ by subtracting the projection of $\ket{w}$ onto $\ket{s}$ from $\ket{s}$:
\begin{equation}
    \label{eq:grover_s'}
    \ket{s'} = \ket{s} - \bra{w}\ket{s}\ket{w}.
\end{equation}
These two orthogonal states form a basis of a two-dimensional subspace of the greater Hilbert space.
This permits a perspicuous visualisation of the algorithm, as in \cref{fig:grover}.
The uniform superposition state $\ket{s}$ serves as a starting point for the algorithm, and is achieved by applying Hadamard gates to all qubits.
It is expressible as
\begin{equation}
    \ket{s} = \cos(\theta)\ket{s'} + \sin{(\theta)}\ket{w},
\end{equation}
where $\theta=\arcsin\bra{s}\ket{w}=\arcsin{}(1/\sqrt{N})$.

Applying the oracle on $\ket{s}$ leaves its $\ket{s'}$ component unchanged, but flips the sign of the $\ket{w}$ component.
This results in the state $\ket{\psi} = \cos(-\theta)\ket{s'} + \sin{(-\theta)}\ket{s'}$, which can be seen as reflection of $\ket{s}$ in the $\ket{s'}$ direction.


Next, the state $\ket{\psi}$ is reflected about the initial $\ket{s}$ state, resulting in the state $\ket{\psi'} = \cos(3\theta)\ket{s'} + \sin(3\theta)\ket{w}$.
Reflection thus is achieved by the diffusion operator
\begin{equation}
    D=H^{\otimes n} S_0 (H^{\otimes n})^{-1} = H^{\otimes n} S_0 H^{\otimes n},
\end{equation}
where $S_0= 2\ketbra{0}{0} - I$ is the reflection operator about the $\ket{0}$ state, that is an operator that flips the sign of all but the $\ket{0}$ component.

The product of the oracle and the diffusion operator defines the Grover operator, which is simply applied until the amplitude of the  $\ket{w}$ is sufficiently amplified.
After $k$ iterations, the state is $\ket{\psi_k} = \cos((2k+1)\theta)\ket{s'} + \sin((2k+1)\theta)\ket{w}$.
Measuring the correct state has probability $\sin^2((2k+1)\theta)$.
Therefore, $k \approx {\pi}/{4\theta}$ iterations should be completed.
Assuming large $N$, for a short list would not warrant the use of Grover's algorithm, $\theta = \arcsin{({1}/{\sqrt{N}})} \approx 1/\sqrt{N}$, and so $k \approx \pi\sqrt{N}/4$.

For lists with more than a single desired element, a similar reasoning will lead to the same algorithm, but instead with $k\approx\pi/4\sqrt{N/M}$, where $M$ is the number solutions to $f(x)=1$~\autocite{nielsen2012}.


\begin{figure}
    \centering
    \begin{tikzpicture}
        \node (O) at (3,3) {\bf{(a)}};
        \def\ang{15}
        \coordinate (O) at (0,0);
        \coordinate (S) at (3,0);
        \coordinate (W) at (0,3);
        \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});

        % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
        \draw[->] (O) -- (W) node[above] {$\ket{w}$};
        % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

        \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
        \draw[->, blue] (O) -- (P) node[right] {$\ket{\psi}$ = $\ket{s}$};
        \pic[
            "$\theta$",
            draw=black,
            angle radius = 1cm,
            pic text options={
                    shift = {(0.8 cm, 0.1 cm)}
                }
        ] {angle = S--O--P};

        \begin{scope}[shift={(6, 0)}]
            \node (O) at (3,3) {\bf{(b)}};
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            \draw[->, blue] (O) -- (P2) node[below] {$\ket{\psi}=\mathcal{O}_f\ket{s}$};
            \pic[
                "-$\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(0.8 cm, -0.1 cm)}
                    }
            ] {angle = P2--O--S};
            \pic[
                <-,
                draw=gray,
                angle radius = 2.5cm,
            ] {angle = P2--O--P};
        \end{scope}

        \begin{scope}[shift={(0, -6)}]
            \node (O) at (3,3) {\bf{(c)}};
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});
            \coordinate (P3) at ({3*cos(3*\ang)}, {3*sin(3*\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            \draw[->] (O) -- (P2) node[below] {$\mathcal{O}_f\ket{s}$};
            \draw[->, blue] (O) -- (P3) node[right] {$\ket{\psi}=D\mathcal{O}_f\ket{s}$};
            \pic[
                "$2\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(0.6 cm, 0.4 cm)}
                    }
            ] {angle = P--O--P3};
            \pic[
                ->,
                draw=gray,
                angle radius = 2.5cm,
            ] {angle = P2--O--P3};
        \end{scope}


        \begin{scope}[shift={(6, -6)}]
            \node (O) at (3,3) {\bf{(d)}};
            \def\ang{7}
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            % \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});
            % \coordinate (P3) at ({3*cos(3*\ang)}, {3*sin(3*\ang)});
            \coordinate (P4) at ({3*cos(12*\ang)}, {3*sin(12*\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            % \draw[->] (O) -- (P2) node[right] {$\ket{\psi}=\mathcal{O}_f\ket{s}$};
            \draw[->, blue] (O) -- (P4) node[right] {$\ket{\psi}=G^k\ket{s}$};
            \pic[
                "$(2k+1)\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(1.2 cm, 0.4 cm)}
                    }
            ] {angle = S--O--P4};
        \end{scope}
    \end{tikzpicture}
    \caption{
        Grover's algorithm visualised.
        (a) The initial uniform superposition state $\ket{s}$ is prepared, which can be seen as a linear combination of $\ket{w}$ and $\ket{s'}$, forming an angle $\theta$ to the $s'$-axis.
        (b) The oracle $\mathcal{O}_f$ is applied to $\ket{s}$, flipping the sign of its $\ket{w}$ component, inverting the angle $\theta$.
        (c) The diffusion operator $D$ is applied, reflecting the state about the initial state and towards the goal, resulting in a state with an angle $3\theta$ to the $w$-axis.
        (d) After repeating the previous two steps a $k$ times, the angle is $2k+1\theta$, and if $k$ is chosen wisely, this means that the system is in a state close to the desired state $\ket{w}$, such that measuring the system will likely result in $\ket{w}$.
    }
    \label{fig:grover}
\end{figure}

\subsection{Amplitude amplification}
\label{sec:amplitude-amplification}
Amplitude amplification can be considered a generalisation of Grover's algorithm.
Instead of a single oracle, let the partitioning of the state space be given by a Hermitian projector $P$, whose image will be the space of states to amplify.
Then, for some given initial state $\ket{\psi}$, it is decomposed into the orthogonal components
\begin{equation}
    \ket{\psi} = \sin(\theta)\ket{\psi_0} + \cos(\theta)\ket{\psi_1},
\end{equation}
where $\ket{\psi_1} = P\ket{\psi}$ and $\ket{\psi_0} = \ket{\psi} - \ket{\psi_1}$, effectively the projections onto the image and kernel of $P$.
Clearly, the angle $\theta$ is given by $\arcsin(|P\ket{\psi}|)$.

The Grover operator is now given by $G=-S_\psi S_P$ where
\begin{align}
    S_\psi & = I - 2\ket{\psi}\bra{\psi} \\
    S_P    & = I - 2P,
\end{align}
such that $S_\psi$ being analogue to the diffusion operator $D$ and $S_P$ being the oracle operator.

Following the same reasoning as in the previous section, the state after $k$ iterations is given by
\begin{equation}
    G^k \ket{\psi} = \sin((2k+1)\theta)\ket{\psi_1} + \cos((2k+1)\theta)\ket{\psi_0},
\end{equation}
meaning that $k \approx \frac{\pi}{4\theta}$ will result in a state close to $\ket{\psi_1}$.

Amplitude amplification can be used to speed up Grover search by using an informed prior rather than a uniform prior.
Furthermore, it is useful as a subroutine in other algorithms, such as finding the number of \enquote{good} states for a Grover search~\autocite{brassard2002}, quantum Monte Carlo methods~\autocite{montanaro2015} and some bandit algorithms to be discussed in what follows.

\subsection{Amplitude estimation}
\label{sec:amplitude-estimation}
As with amplitude amplification, amplitude estimation considers states that are decomposed into a superposition of two states, and, as the name suggests, estimates the amplitude of one of the states.
Given a state, or more generally the algorithm, $\mathcal{A}$ with which it is generated,
\begin{equation}
    \mathcal{A} \ket{0} = \sqrt{a} \ket{\psi_1} + \sqrt{1-a} \ket{\psi_0},
\end{equation}
where $\ket{\psi_1}$ is a state of interest and $\ket{\psi_0}$ its orthogonal complement, the goal is to estimate its amplitude $a = \lvert\bra{\psi_1}\ket{\psi_1}\rvert^2$.

With its original formulation in~\autocite{brassard2002}, it was proven that the amplitude can be estimated with an additive error of
\begin{equation}
    \epsilon \leq 2\pi \frac{\sqrt{a(1-a)}}{t} +\frac{\pi^2}{t^2}
\end{equation}
with probability at least $8/\pi^2(\approx 81.06\%)$ using $t$ calls to the algorithm $\mathcal{A}$.
The probability can be increased to $1-\delta$, requiring $O(\log(1/\delta))$ calls to $\mathcal{A}$~\autocite{montanaro2015}.
Later variants have been proposed to reduce the qubits needed and circuit depths~\autocite{suzuki2020, nakaji2020, grinko2021}, making it more feasible for NISQ devices, while still achieving similar asymptotic error bounds.


\subsection{Quantum Monte Carlo}
\label{sec:qmc}
Monte Carlo methods have been a powerful tool to analyse the behaviour of quantum mechanical systems, where probabilistic methods are natural to describe probabilistic physics~\autocite{ceperley1986, austin2012,gubernatis2016}.
On the other hand, more in line with the scope of this report, recent advances in quantum computing have opened up a new avenue for the intersection of quantum mechanics and Monte Carlo methods.

In the problem of estimating the mean of a random variable without assumptions of its distribution or properties, the additive error can be bounded by Chebyshev's inequality as
\begin{equation}
    P(\lvert \hat\mu - \mu \rvert \geq \epsilon) \leq \frac{\sigma^2}{n \epsilon^2},
\end{equation}
where $\hat\mu$ is the sample mean, $\mu$ is the true mean, $\sigma$ is the standard deviation and $n$ is the number of samples.
Consequently, there is a need of quadratically many samples to achieve a given error, which for example means that estimating the mean of a random variable with a standard deviation of $1$ with four decimals' accuracy and a certainty of $99.9\%$ would require $10^9$ samples.
Moreover, this is provably optimal asymptotically~\autocite{dagum2000}.

Generalising amplitude estimation, in~\autocite{montanaro2015}, a \enquote{near-quadratic} speed-up of Monte Carlo methods was achieved by using amplitude estimation to estimate the mean of a random variable encoded by quantum algorithms.
Given an algorithm $\mathcal{O}$ whose measurement outputs are assigned real values such that $v(\mathcal{A})$ is a random variable with mean $\mu$ and variance $\sigma^2$, it is proved that approximating $\mu$ with an additive error of $\epsilon$ can be achieved with only $\tilde{O}(\sigma/\epsilon)$ calls to $\mathcal{A}$ and its inverse, which is a near-quadratic speed-up over the classical case\footnotemark{}.

\footnotetext{
    The $\tilde{O}$ notation ignores logarithmic factors.
}

The simpler version on which the general builds, $v(\mathcal{A})$ is assumed to lie in the interval $[0,1]$.
Thus, the value can be encoded in a single qubit by through a unitary
\begin{equation}
    U\ket{x}\ket{0} = \ket{x}\left(\sqrt{1 - \phi(x)}\ket{0} + \sqrt{\phi(x)}\ket{1}\right),
\end{equation}
where $\phi(x)$ is the output of the algorithm were $x$ to be measured.
Thence, the amplitude of the last qubit is simply estimated using amplitude estimation, as described in \cref{sec:amplitude-estimation}, using an appropriate number of iterations and repetitions.
The initial state for the amplitude estimation is set to
\begin{equation}
    \ket{\psi} = U(\mathcal{O} \otimes I)\ket{0}.
\end{equation}

For these bounded random variables in particular, $O(1/\epsilon)$ iterations suffices to achieve an additive error of $\epsilon$, repeating the whole procedure $O(1/\log(\delta))$ times to achieve a certainty $1-\delta$~\autocite{montanaro2015}.