\section{Quantum neural networks}
\label{sec:qnn}
Quantum neural networks (QNNs) are simply an abstraction of parametrised quantum circuits with some sort of data encoding.
As classical artificial neural networks have made classical machine learning into a powerful tool, QNNs are envisioned as a quantum counterpart, inheriting some classical theory, nomenclature and perhaps unfounded hype.
The main goal of QNNs is to do what classical NNs do, but with some quantum advantage, be it in terms of generalisability, training required or something else.

The structure of most quantum neural networks follow classical feed-forward networks.
\Cref{fig:qnn} shows the general circuit layout.
In the first step (or layer), data is encoded into the qubits, typically using a method discussed in \cref{sec:data_encoding}.
Next, the data is passed through a sequence of parametrised quantum gates which often can be interpreted as belonging to layers.
Lastly, an output is produced by measuring the qubits, potentially with some post-processing.
Thence, a cost function is calculated, and the parameters are updated.

\begin{figure}[tbp]
  \centering
  \begin{quantikz}
    \lstick{$\ket{0}$} &
    \gate[wires=4, nwires=3]{\text{Encoding}(\bm{x})} &
    \gate[wires=4, nwires=3]{U_1(\bm{\theta})}
    \gategroup[
      wires=4,
      steps=3,
      style={dashed, rounded corners, inner sep=2pt},
      label style={label position=below, anchor=north, yshift=-0.2cm}
    ]{Variational circuit $U(\bm{\theta})$} &
    \ \ldots\ \qw &
    \gate[wires=4, nwires=3]{U_n(\bm{\theta})} &
    \meter{}
    \\
    \lstick{$\ket{0}$} & & & \ \ldots\ \qw & & \meter{}
    \\
    \lstick{\vdots} & & & & &
    \\
    \lstick{$\ket{0}$} & & & \ \ldots\ \qw & & \meter{}
  \end{quantikz}
  \caption[
    General structure of quantum neural networks.
  ]{
    General structure of quantum neural networks.
    First, some data $\bm{x}$ is encoded into a state $\ket{\psi(\bm{x})}$ using some encoding strategy.
    Then, the state is transformed by a parametrised quantum circuit $U(\theta)$.
    This variational circuit needs to be decomposed into a sequence of gates $U_1,\dots, U_n$, making the QNN structure more akin to the layered classical neural networks.
    These gates or layers do not need to use all qubits, but can be restricted to a subset, mimicking the classical concept of differently sized hidden layers.
    Finally, measurements are made and used to calculate the model output.
  }
  \label{fig:qnn}
\end{figure}

In~\autocite{abbas2021}, it was shown that QNNs can have higher expressibility and be easier to train than comparable classical NNs.
What is more, several particular architectures inspired by classical NNs have evinced advantages over their classical forerunners, as will be seen in \cref{sec:qnn_arch}.
Withal, any intrinsic quantum advantage is still to be proven, and there are challenges that must be overcome for QNNs to be useful in practice.

\subsection{Data encoding}
\begin{table}
  \centering
  \captionabove[
    Properties of different data encodings.
  ]
  {
    Properties of different data encodings.
    Given an $N$-dimensional data set of $M$ data points, the qubits needed is a lower bound for qubits required to represent the data, and circuit depth is the number of gates needed for the encoding algorithm.
    For basis encoding, $b(N) \geq N$ is the number of bits needed to represent an $N$-dimensional data point, for instance by using floating point representations of continuous data.
  }
  \label{tab:data_encoding}
  \begin{tblr}{
      width=\linewidth,
      colspec={Q[l,m,co=1] Q[c,m,co=1] Q[c,m,co=1] Q[c,m,co=1]},
    }
    \toprule
    {Encoding strategy}           & {Qubits needed}         & {Circuit depth} & {Hard to simulate classically} \\ \midrule
    Basis encoding                & $b(N)$                  & $O(N)$          & No                             \\ %\cmidrule{1-4}
    Amplitude encoding            & $\lceil\log_2{N}\rceil$ & $O(N)$          & Yes                            \\ %\cmidrule{1-4}
    Angle encoding                & $N$                     & $O(N)$          & No                             \\ %\cmidrule{1-4}
    {Second order angle encoding} & $N$                     & $O(N^2)$        & {Yes? (Conjectured)}           \\ \bottomrule
  \end{tblr}
\end{table}
\label{sec:data_encoding}
In order for quantum computers to use classical data, it must first be encoded in a way that is compatible with the quantum hardware.
How this is done has major implications on both the computational performance and the model expressibility.
While na√Øve techniques like basis encoding are possible and easy to understand, more complex procedures are often needed to achieve good performance.
The four methods that will be discussed in this section are summarised in \cref{tab:data_encoding}.




\subsubsection{Basis encoding}
The perhaps simplest way to encode data is to use the computational basis states of the qubits.
This is done in the same way that classical computers use binary numbers.
For example, some data $x$ can be expressed as a bit-string $x = \{x_1, x_2, \dots, x_n\}$, where each $x_i$ is either 0 or 1, where any continuous variables are encoded as floating point numbers.
For multidimensional data, the bit-strings are simply concatenated.
% 
If for instance the data point $010101$ is to be encoded in a quantum computer, it is simply mapped to the computational basis state $\ket{010101}$.
This allows for multiple data points to be encoded in parallel as
\begin{equation}
  \ket{\mathcal{D}} = \frac{1}{\sqrt M} \sum_{m=1}^M \ket{\bm{x}^{(m)}},
\end{equation}
where $\mathcal{D}$ is the data set, $M$ the total number of data points and $\bm{x}^{(m)}$ the $m$-th binarised data point.
This is a simple encoding and has some significant disadvantages.
There must be at least as many qubits as there are bits in the binarised data.
For $N$ bits, there are $2^N$ possible states, but at most $M$ are used, which means that the embedding will be sparse.
This means that the computational resources required to encode the data will in some sense wasted, and that the quantum computer will not be able to exploit the full power of the quantum hardware.
To utilise the entire Hilbert space, amplitude encoding is better suited.

\subsubsection{Amplitude encoding}
A more efficient way to encode data is to use amplitude encoding, exploiting the exponentially large Hilbert space of quantum computers.
This is done by mapping the bits in the bit-string not to individual qubits, but to individual amplitudes in the exponentially large Hilbert space.
Mathematically, for some $N$-dimensional data point $\bm{x}$, this reads
\begin{equation}
  \ket{\psi(\bm{x})} = \sum_{i=1}^{N} x_i \ket{i},
\end{equation}
where $x_i$ is the $i$th component of the data point and $\ket{i}$ is the $i$th computational basis state.
This has the advantage of being able to encode any numeric type natively, and perhaps more importantly, only needing logarithmically many qubits.
For $N$-dimensional data points, only $\lceil \log_2 N \rceil$ qubits are needed.
This is a significant improvement over the basis encoding, which requires $N$ qubits (or more if integers and floats are to be binarised).

Amplitude encoding can easily be extended to cover the entire data set.
This is done by concatenating the data points, after which the data set $\mathcal{D}$ with $M$ data points can be encoded as
\begin{equation}
  \ket{\mathcal{D}} = \sum_{m=1}^M \sum_{i=1}^{N} x_i^{(m)} \ket{i} \ket{m},
\end{equation}
where $x_i^{(m)}$ is the $i$th component of the $m$th data point.
For such encodings, only $\lceil \log_2 (N M) \rceil$ qubits are needed.

There are two main drawbacks of amplitude encoding.
First, that the data must be normalised, which can be done without loss of information by requiring an additional bit to encode the normalisation constant.
Also, some padding may be needed if the dimension of the data is not a power of two.
Secondly and more severely, there are significant practical difficulties with preparing such states.
Any state of the form
\begin{equation}
  \ket{\psi} = \sum_{i} a_i \ket{i}
\end{equation}
must be efficiently and correctly prepared, which is not trivial.
Unless some very specific assumptions are made, this is not possible with polynomially many gates (as a function of the number of qubits), which limits the potential for exponential speed-ups~\autocite{schuld2018}.
In general, for classical data, circuits must be linearly deep in the size of the data and ergo exponentially deep in the amount of qubits, which makes it beyond the reach of NISQ hardware.

\subsubsection{Angle encoding}
A third option is to encode data into the angles of rotations.
Here, the potentially continuous components of the data are mapped to rotations of the qubits.
For the rotations to be meaningful angles and not loop around, the data need to be normalised.
An $N$-dimensional data point $\bm{x}$ is then encoded as
\begin{equation}
  \ket{\psi(\bm{x})} = \bigotimes_{i=1}^{N} R_\sigma(x) \ket{0},
\end{equation}
where $\sigma$ can be chosen to be either $X$, $Y$ or $Z$.
For $Z$-rotations, a Hadamard gate is prepended for the rotation to have an effect.
$N$ qubits are still required, but with native support for continuous variables, angle encoding can require fewer qubits than basis encoding.
A constant number of gates are needed to prepare the state, which is a significant advantage over amplitude encoding.
Still, being a product state, it offers no inherent quantum advantage.

\subsubsection{Second order angle encoding}
\label{sec:second_order_angle_encoding}
Conjectured to be hard to simulate classically, a second-order angle encoding is proposed in~\autocite{havlicek2019}.
First, angles are encoded as above, but thereafter the qubits are entangled and rotated further based on second order terms.
In circuit notation, such an encoding with $Z$-rotations reads
\begin{equation}
  \begin{quantikz}[column sep=3mm]
    \lstick{$\ket{0}$} & \gate{H} & \gate{R_Z^1} & \ctrl{1} & \qw & \ctrl{1} & \ctrl{2} & \qw & \ctrl{2} & \qw & \qw & \qw & \qw & \dots \\
    \lstick{$\ket{0}$} & \gate{H} & \gate{R_Z^2} & \targ{} & \gate{R_Z^{1,2}} & \targ{} & \qw & \qw & \qw & \ctrl{1} & \qw & \ctrl{1} & \qw & \dots \\
    \lstick{$\ket{0}$} & \gate{H} & \gate{R_Z^3} & \qw & \qw &  \qw &  \targ{} & \gate{R_Z^{1,3}} & \targ{} & \targ{} & \gate{R_Z^{2,3}} & \targ{} & \qw & \dots \\
    \lstick{\vdots} \\
    \lstick{$\ket{0}$} & \gate{H} & \gate{R_Z^N} & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \dots
  \end{quantikz},
  \label{eq:second_order_encoding}
\end{equation}
where $R_Z^i = R_Z(x_i)$ and $R_Z^{i,j} = R_Z((\pi-x_i)(\pi-x_j))$ and with the entanglements and second-order rotations being applied pairwise for all $N$ qubits.
This increases the circuit depth to order $N^2$, and full connectivity is needed.
Nonetheless, the increased circuit depth could be compensated for by the extra entanglement and subsequent expressibility, compared to the first order encoding.
Were it indeed classically hard to simulate, it could provide quantum advantage.

\subsubsection{Repeats}
The expressive power of models heavily rely on the encoding strategy.
For instance, a single qubit rotation only allows the model to learn sine functions, where the frequency is determined by the scaling of the data.
Generally, quantum models will learn periodic functions, and thus Fourier analysis is a useful tool.
The implications of this is studied in~\autocite{schuld2021}, where it is shown that simply repeating simple encoding blocks allows for learning of more frequencies and thus more complicated functions.
Asymptotically, such repeats lets a quantum model learn arbitrary functions.


\subsection{Challenges}
While the power of classical neural network relies on the non-linear activation functions, the unitary operations in quantum computing are inherently linear.
However, depending on how the data is encoded, the linear transformations in the Hilbert space may not be linear in the input space.
With basis encoding, for example, mapping $\ket{b}\ket{0}$ to $\ket{b}\ket{\sigma(b)}$ is doable, where $b$ is a bit-string and $\sigma$ some function.
Amplitude encoding, on the other hand, has its input necessarily transformed linearly, and is for this reason (in addition to those mentioned in \cref{sec:data_encoding}) less suitable for QNNs.
Such linear transformations in an embedding space can indeed be useful, both in the classical and quantum settings, but they may not be sufficient to make QNNs as powerful as classical NNs.
It may make more sense to consider those models as kernel methods instead of neural networks, for which some quantum advantage has been indicated~\autocite{schuld2019a}, but is beyond the scope of this report.

If intermediate measurements are used, non-linearities can be introduced in the quantum circuit.
One way of doing this includes controlling gates with measurement results, as in~\autocite{cong2019}.
Another way is so-called repeat-until-success schemes~\autocite{cao2017}, where the circuit is run until the measurement of one qubit is what is desired before the remaining state can be used for further computation.
Mid-circuit measurements can be used to define non-linear quantum neurons~\autocite{yan2020}.
There, qubits are grouped together to represent one neuron, and intermediate measurements are used to approximate activation functions with piecewise constant functions.

Though there are methods like the parameter-shift rule that make it possible to find gradients and train the network using classical methods, none are as efficient as classical backpropagation for classical neural networks.
This is because these methods require separate evaluations of the circuit, including numerous shots, for each parameter, whereas backpropagation is easily done after a forward pass through the network.
Add to this the problems of noise and vanishing gradients, discussed in \cref{sec:nisq,sec:vqa}, and it is clear that NISQ-era QNNs can not be expected to scale as well as classical NNs do.


\subsection{Architectures}
\label{sec:qnn_arch}
Many architectures for QNNs have been proposed, commonly inspired by those for classical NNs.
Still being in its infancy, it is not clear which (if any) will prove useful.
Below follow some promising architectures, with a brief description of how they work compared to classical neural networks\footnote{Q.v. \cref{sec:nn}.}.

\subsubsection{Quantum convolutional neural networks}
\label{sec:qcnn}
Originally introduced in~\autocite{cong2019}, quantum convolutional neural networks (QCNNs) take inspiration from classical convolutional neural networks in that a sequence of convolutional and pooling layers are used to extract features and reduce the dimension before an output is made.
In the quantum convolutional layers, neighbouring qubits are entangled by some parametrised gates, after which pooling layers reduce the active qubit count (usually by half).
By basing the pooling on measurements, controlling a gate based on a neighbouring qubit measurement, non-linearities are introduced.
Because of the constant reduction of layer sizes in (Q)CNNs, the total parameter count can be reduced to only logarithmic order of the network depth, making them easier to train than dense networks of similar input size.
After several iterations of convolution and pooling, gates can be employed on the remaining qubits, analogous to a finishing fully connected layer in classical CNNs, before the final
measurement and output.

QCNNs have been shown to be able to classify topological phases of matter~\autocite{cong2019} and that they inherit their classical counterparts' ability to classify images~\autocite{oh2020}.
Also, QCNNs have desirable properties with regard to avoiding barren plateaus~\autocite{pesah2021}, which could prove essential in training for problems of interesting size.

\subsubsection{Quantum generative adversarial networks}
Quantum generative models have been shown potentially to have an exponential advantage over their classical counterparts~\autocite{gao2018}.
Due to the inherent probabilistic nature of quantum machines, it should not be surprising that they could learn difficult distributions more naturally than classical computers do.
Moreover, leveraging a classical model as the adversary ensures that the quantum model can be of reasonable scale.
Real quantum hardware has been used to generate (admittedly low-resolution) images of handwritten images~\autocite{huang2021}.

\subsubsection{Hybrid quantum-classical neural networks}
Another option is to include a quantum layer or node is some larger pipeline or even non-linear graph structure.
As parametrised quantum circuits are differentiable in their parameters, they can be handled using the chain rule when backpropagating a hybrid model.
In~\autocite{killoran2019}, several such models are described and tested.
The authors note that for the NISQ-era, limiting quantum components of models to very particular tasks to which they are especially suited should be beneficial.
As quantum hardware develops, they can take over more and more of the hybrid models.

Quanvolutional neural networks, proposed in~\autocite{henderson2020}, are a hybrid model in which the convolutional layers of a classical CNN are replaced by quantum layers.
As the quantum part is restrained to a single layer and a small convolutional kernel, the design can be implemented with small quantum circuits with little requirements for error-mitigation, still being able to process high-dimensional data, thereby making it a good candidate for NISQ-era hardware.

More recently, in~\autocite{zeng2022}, using a hybrid model for multi-class classification on real world data sets using a CNN-inspired structure was explored.
In the model used, the quantum part was placed in the middle, after classical convolutions and pooling and before a classical fully connected layer.
There, it was shown that the hybrid model could outperform a classical CNN of similar parameter size.

\subsubsection{Quantum recurrent neural networks}
The quantum state is collapsed upon measurement, losing information in the process.
There is therefore not straightforward to translate the classical recurrent neural network (RNN) to a variational quantum circuit in the manner of \cref{fig:qnn} and the above discussed architectures.
Still, there attempts have been made to define a QRNN, such as in~\autocite{bausch2020} where parametrised quantum neurons are defined and used to construct layers.
These layers have two sets of qubits.
The first set is not measured, and the state persists to the next layer.
In contrast, the second set consists of ancillary qubits, which are either initiated in the state $\ket{0}$ or used to input data.
The ancillary qubits are measured after each layer and interpreted as the output of the layer.
It is shown to achieve 95\% accuracy on classifying 0 and 1 in the MNIST data set with only 12 qubits.

In~\autocite{takaki2021}, a similar design for QRRNs is used to learn temporal patterns in data, such as cosine waves or spin quantum dynamics.