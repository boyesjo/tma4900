\chapter{Final remarks}
\label{chap:final}

\section{Conclusions}
This thesis has explored the multi-armed bandit problem and the application of quantum computing to solve it.
Moreover, the application of modern reinforcement learning has also been explored, with the aim of solving the bandit problem using general reinforcement learning algorithms of both classical and quantum nature.
The bandit problem is a widely applicable and useful problem that has been studied for many decades.
As discussed in the introduction, it is indeed useful to real-world problems, and with computers automating yet more tasks, ensuring these are well-informed is pertinent.
Despite its simplicity, the bandit problem remains intractable to solve optimally in most cases.
In fact, defining precisely what an \enquote{optimal} policy is for the bandit problem is a non-trivial task, and three main definitions are discussed in this thesis, namely instance-optimality, minimax-optimality and Bayesian optimality with respect to some prior distribution.
Still, classical algorithms that achieve optimal performance with respect to either of these, though often only asymptotically, do exist.
Perhaps most notable of these is the Thompson sampling algorithm, which is asymptotically instance-optimal and nearly minimax-optimal, and which tends to perform well in practice.
Research in stochastic bandits has been active for decades, but is still very much ongoing.

Quantum computing has the potential to solve certain problems that are intractable to solve classically.
Based on quantum Monte Carlo methods to estimate the mean of random variables with better precision than is classically permitted, the quantum upper bound algorithm proposed in~\autocite{wan2022} that is shown to achieve logarithmic minimax regret, something which is provably impossible classically, has been explained, implemented and studied.
Simulating quantum computing is exponentially hard, but something that can be done with powerful classical hardware without any approximations.

The simulations done for this project show that the quantum upper confidence bound algorithm is only supreme on what can be considered \enquote{hard} instances of the bandit problem, that is those in which reward means are close and for which many samples will be needed to thoroughly distinguish between them.
For easy instances, where expected rewards are far apart, the algorithm is, in essence, wasting turns on sampling and producing unnecessarily accurate estimates of the reward means.
It is subsequently significantly outperformed by the classical Thompson sampling algorithm.
So far, the quantum algorithm had not been compared against Thompson sampling, such that highlighting the cases in which QUCB performs relatively poorly is here a novel contribution.

The algorithm's difficulty-dependent performance should not be entirely unexpected, as the algorithm relies on exponentially many consecutive samples to achieve its superior predictive performance, and in the easy instances, arms must be eliminated much earlier to achieve the lowest reasonable regret.
The provably improved regret bound is no guarantee of improved performance in practice, being both a bound rather than a guarantee and asymptotic in nature.
The algorithm is not a panacea for the bandit problem.

There were some arbitrary choices made in the implementation of the algorithm, primarily in the choices made for the QUCB hyperparameters.
Different choices may be able to improve the algorithm's performance, but its overall behaviour is unlikely to change.
Still, it would be interesting to study the effects of different choices of hyperparameters.

While the quantum upper bound algorithm may be useful for certain problems, it can not be applied to any bandit problem.
Its quantum Monte Carlo subroutine relies on the rewards being observed as quantum mechanical superpositions corresponding to the reward distributions.
For most, if not all, of the applications discussed in this thesis, this can not be achieved.
\enquote{(\dots) human participants, unlike computer programs, cannot be queried in superposition.}~\autocite{wang2021}.
This does not mean that the QUCB algorithm is purely an academic curiosity, however.
Were quantum computing and quantum communication to advance to an interesting scale, quantum bandit algorithms could be used to enhance quantum computing-based algorithms in the same way classical bandit algorithms are used classically.

Modern reinforcement learning has shown the ability to solve some problems better than humans can.
With neural networks and increasingly powerful computers, state-of-the-art methods can beat even the best humans at board games such as Go and video games like \textit{Starcraft~II} and \textit{Dota~2}.
Combining quantum computing with reinforcement learning has the goal of elevating the performance of RL algorithms to a new level, and recent results have indeed shown some signs of variational quantum algorithms having certain advantages over classical algorithms.
This is particularly true for problems with inherent quantum properties.

Attempts to solve the bandit problem with both classical reinforcement learning and quantum reinforcement learning left lacklustre results.
It may be that these algorithms require proper tuning and design of both reward functions and state representations, as is often the case with RL problems.
Nonetheless, it shows that bandit-specific algorithms are much better suited than general-purpose ones, despite the power provided by neural networks, quantum neural networks and great parameter counts.

In conclusion, this thesis has shown that the multi-armed bandit problem is a challenging and important problem that has been studied for many years.
The application of quantum computing and reinforcement learning to bandit problems has shown promise, but more research is needed to determine their effectiveness in practice.
The development of purpose-designed algorithms is crucial to achieving optimal performance in real-world applications.

\section{Outlook}
The pursuit of optimal policies for the multi-armed bandit problem remains a perennial subject of research.
The algorithms explicated in this thesis, though applicable and theoretically sound, represent merely a fraction of the voluminous literature devoted to bandits.
The fundamental, but basic stochastic bandit case is but the tip of the iceberg, as there exist a plethora of generalisations and variations, many of which provide even more useful tools to be applied in practice.
Among these are contextual bandits, adversarial bandits, combinatorial bandits, linear bandits and many more.
How the methods presented here may be extended to these cases is an interesting question.

By introducing superposition as a permissible mode of querying the arms, it is obvious that advantages will be gained.
However, it is clear that there exist lower bounds which place limits on the performance attainable through these quantum bandit formulations.
It may thus be an interesting pursuit to investigate the nature of these quantum lower bounds, both for minimax- and instance-optimality.

Furthermore, being limited to arms that provide superpositional rewards, it is a daunting task to apply quantum algorithms.
As such, it would be of great interest to find any explicit scenarios wherein these algorithms could be used and provide real benefits.
Could some quantum reinforcement learning algorithms be made more efficient with QUCB?

The quantum bandit model considered in this thesis, where arms are pulled by applying corresponding oracles, is merely one of many plausible quantum bandit schematics.
Several others have been proposed thus far, as touched upon in \cref{sec:qbandits_other}, some of which may be more amenable to practical applications.
These have necessarily other properties, and it would be interesting to study what quantised bandits be the most useful in practice.

The impressive strides taken in the field of reinforcement learning bear witness to the potential efficacy of applying quantum computing to this discipline.
Nevertheless, it is plagued by the difficulties inherent in quantum machine learning more generally, whereby empirical demonstrations of real benefits remain elusive and intricate practical problems are often too complicated for exact, analytical considerations.
Still, it is undoubtedly a promising field of research.

In future work, one may investigate whether the classical and quantum reinforcement learning algorithms considered here are truly incapable of producing a satisfactory solution to the bandit problem, or whether they simply necessitate more design and fine-tuning, which time constraints precluded here.
It would also be interesting to investigate the use of other quantum reinforcement learning algorithms, such as a quantum value-based algorithm or some actor-critic algorithm where either or both are quantised.
Overall, the study and comparison of these quantum reinforcement learning algorithms will be a fruitful area of research.
