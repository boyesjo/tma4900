\chapter{Multi-armed bandits}
\label{chap:bandits}

The multi-armed bandit (MAB) problem is a classic problem in reinforcement learning and probability theory.
It poses a simple yet challenging problem, where the agent must sequentially choose between a number of arms (distributions) of which the mean reward is unknown, trying to maximise the cumulative reward.
This poses a constant struggle between exploration and exploitation, where exploration is the process of trying out new arms, and exploitation is the process of sampling from the distribution with the highest average thus far.

Bandit problems were first considered in 1933 \cite{thompson1933}, but the term was not coined before 1952 \cite{robbins1952}.
Some of the many real-world setting where the multi-armed bandit problem is applicable are listed in \cref{tab:mab_applications}.
Despite being a simple problem, its countless variants and applications make it not only a useful tool for real-world problems.
Netflix uses MAB theory to recommend movies \cite{kawale2018}, Amazon for its website layout \cite{hill2017}, Facebook for video compression \cite{daulton2019} and Doordash to identify responsive deliverymen \cite{sharma2022}.
The problem and its variations are still being studied with several results in what follows being recent.

\begin{table}
    \centering
    \caption{
        Some applications of the multi-armed bandit problem.
    }
    \label{tab:mab_applications}
    \begin{tblr}{
            width=\linewidth,
            colspec={Q[l] Q[r] Q[r]}
        }
        \toprule
        Application            & Arms                 & Reward            \\
        \midrule
        Medical trials         & Drugs                & Patient health    \\
        Online advertising     & Ad placements        & Number of clicks  \\
        Website design         & Layouts/fonts \&c.   & Number of clicks  \\
        Recommendation systems & Items                & Number of clicks  \\
        Dynamic pricing        & Prices               & Profit            \\
        Networking             & Routes, settings     & Ping              \\
        Lossy compression      & Compression settings & Quality preserved \\
        Tasking employees      & Which employee       & Productivity      \\
        Finance                & Investment options   & Profit            \\
        \bottomrule
    \end{tblr}

\end{table}


\section{Formal definition}
In the multi-armed bandit problem, a set of $k$ distributions are given, each with an unknown mean $\mu_i$.
Denote the set of distributions by $\mathcal{A}$ and the distributions by $P_i$, $i \in \{1,2,\dots,k\}$.
These distributions have unknown means $\{\mu_1, \mu_2,\dots,\mu_k\}$.
The agent is given a number of turns $T$, known as the (time-) horizon, and at each turn $t$, the agent selects an arm $i_t$ whence it samples independently of previous samples, receiving a reward $r_{i_t} \sim P_{i_t}$.
The goal is to maximise the expected sum of rewards.

To summarise:
\begin{description}
    \item[Known:] Number of arms $k$, number of rounds $T$
    \item[Unknown:] Mean rewards $\{\mu_1, \mu_2,\dots,\mu_k\}$, distributions $\{P_1, P_2,\dots,P_k\}$
    \item[Game:] For $t=1,\dots,T$: select arm $i_t$ and receive reward $r_{i_t} \sim P_{i_t}$ (independently of previous samples)
    \item[Goal:] Maximise $E(\sum_{t=1}^T r_{i_t})$
\end{description}

While the mean rewards always are unknown, some assumptions will be made about the distributions, of which some common ones are listed in \cref{tab:mab_assumptions}.
Naturally, stronger assumptions lead to better algorithms.


\subsection{Performance}
For the analysis of algorithm performance, the regret is used.
At round $T$, the regret is defined as
\begin{equation}
    R(T) = \sum_{t=1}^T \mu^* - \mu_{i_t},
\end{equation}
where $\mu^*$ is the highest, optimal mean, and $\mu_{i_t}$ is the mean of the arm selected at time $t$.
Denoting by $N_i(T)$ the number of times arm $i$ is pulled and $\Delta_i$ the difference between the mean of arm $i$ and the optimal mean, the regret can be rewritten as
\begin{equation}
    R(T) = \sum_{i=1}^k N_i(T) \Delta_i.
\end{equation}
Algorithms are often probabilistic, so often the expected regret is of more interest, given by
\begin{equation}
    E(R(T)) = \sum_{i=1}^k E(N_i(T)) \Delta_i.
\end{equation}
Regret will be used interchangeably with expected regret, with the meaning being clear from the context.
When discussing algorithms in general, the expected regret is of concern, while considering particular simulations, the regret is of interest.
The number of turns $T$ is often referred to as the (time-) horizon and will be assumed to be greater than $k$, such that all arms may be pulled.
While $T$ is assumed given, many algorithms are designed to work independently of it, being known as anytime algorithms.

\begin{table}
    \centering
    \caption{
        Common assumptions made about MAB distributions.
    }
    \label{tab:mab_assumptions}
    \begin{tblr}{
            width=\linewidth,
            colspec={Q[l] Q[r]}
        }
        \toprule
        Assumption                 & Description                                                       \\
        \midrule
        Bernoulli                  & $r \sim \mathcal{B}(\mu)$                                         \\
        Gaussian, unit variance    & $r \sim \mathcal{N}(\mu,1)$                                       \\
        Gaussian, unknown variance & $r \sim \mathcal{N}(\mu,\sigma^2)$                                \\
        Sub-Gaussian               & $\exists C > 0: P(|r| \geq \epsilon) \leq 2\exp(-\epsilon^2/C^2)$ \\
        Bounded value              & $r \in [0,1]$                                                     \\
        \bottomrule
    \end{tblr}
\end{table}





\subsection{Optimality}
In the realm of multi-armed banditry, expressing optimality is fraught with difficulties.
Any precise formulation is contingent upon not only the assumptions made, but also the particular instance, namely actual means and any other parameters.

In order to meaningfully define a lower bound, it is imperative to assume a reasonable algorithm.
Otherwise, trivial policies, such as always pulling the first arm, would achieve zero regret, hindering any meaningful comparison.
One may therefore impose asymptotic consistency, which by definition means that for all inferior arms and all $\alpha \in (0, 1)$,
\begin{equation}
    E(R(T)) = o(T^{\alpha}),
\end{equation}
where $N_i(T)$ is the number of times arm $i$ have been pulled in the first $T$ turns.
For algorithms obeying this property, the Lai-Robbins bound holds \cite{lai1985}, namely that
\begin{equation}
    \lim_{T\to\infty} \inf \frac{E(N_i(T))}{\ln T} \geq \frac{1}{D(P_i || P^*)},
\end{equation}
where $P_i$ is the reward distribution of arm $i$, $P^*$ that of the optimal distribution and $D(\cdot || \cdot)$ is the Kullback-Leibler divergence.
Effectively then, any asymptotically consistent algorithm will suffer a regret of at least $\Omega(\ln T)$.

The Lai-Robbins bound is instance-dependent through its dependence on the Kullback-Leibler divergences.
An alternative bound is proved in \cite{slivkins2019}, states that for all algorithms, given a fixed horizon $T$ and number of arms $K$, there is at least one problem instance such that
\begin{equation}
    E(R(T)) \geq \Omega(\sqrt{KT}).
\end{equation}
This is truly a worst-case bound; good algorithms will typically perform much better.


\subsection{Best-arm identification}
An alternative problem is to find the best arm with as few turns as possible.
In this version, a $\delta$ is given, and the goal is to find the best arm with probability at least $1-\delta$.
The metric here is how the turns needed grows with $\delta$.
Unlike regret minimisation, exploitation is less of a concern, but much theory can be transferred from the regret minimisation problem.
Though less is gained from repeatedly pulling the assumed best arm, there is still incentive to investigate the better arms than the worse.

\subsection{Bandit generalisations}
The multi-armed bandit problem has numerous generalisations, including the non-stationary multi-armed bandit where the underlying reward distributions change over time, presenting a challenging environment for traditional algorithms developed for the standard, stationary multi-armed bandit problem.
In this variant, agents must continuously explore and adapt to the changing environment.
Other cases give the agent more info, such as letting it know what the rewards for all arms, were they pulled instead, would have been.
Another area of study is the contextual multi-armed bandit problem, in which contextual information must be incorporated into the decision-making process for arm selection, adding a layer of complexity to the standard multi-armed bandit problem, particularly useful for recommender systems, where the context is the user and their preferences.
Moreover, the adversarial multi-armed bandit problem represents a significant departure from the standard, stochastic multi-armed bandit problem, where rewards are chosen by an adversary instead of following a stationary distribution.
The infinite-armed variants, where the arm space is infinite but constrained by for example linearity, also have practical applications.
While beyond the scope of this report, these generalisations of the multi-armed bandit problem represent important areas of study and much of the theory developed for the standard, stochastic multi-armed bandit problem can be extended to these problems as well\footnotemark.
\footnotetext{
    C.v. \cite{slivkins2019,lattimore2020}.
}


\section{Strategies}
\begin{table}
    \centering
    \caption{
        Comparison of strategies.
    }
    \label{tab:strategies}
    \begin{tblr}{
            width=\linewidth,
            colspec={Q[l] Q[r] Q[r] Q[r]}
        }
        \toprule
        Strategy       & Regret      & Tuning    & Ease of implementation \\
        \midrule
        Random         & Linear      & NA        & Easy                   \\
        Greedy         & Linear      & NA        & Easy                   \\
        Epsilon-greedy & Linear      & Difficult & Easy                   \\
        Epsilon-decay  & Logarithmic & Difficult & Easy                   \\
        UCB            & Logarithmic & Barely    & Medium                 \\
        Thompson       & Logarithmic & Priors    & Harder                 \\
        \bottomrule
    \end{tblr}
\end{table}

\subsection{Explore-only}
Pure exploration is obviously a suboptimal strategy, but it is a good baseline against which to compare.
It can be implemented by selecting an arm uniformly or in order, but it will perform poorly either way.
The arm-selection procedure is described by \cref{alg:random}.
\begin{algorithm}
    \caption{Random arm selection}
    \label{alg:random}
    \begin{algorithmic}
        \State Sample $i$ from $\{1,\dots, k\}$ uniformly
        \State \Return $i$
    \end{algorithmic}
\end{algorithm}

It is easy that the expected regret is
\begin{equation}
    R(T) = T\left(\mu^* - \frac{1}{k}\sum_{i=1}^k \mu_i\right),
\end{equation}
which is necessarily linear in $T$.
This motivates the search for an algorithm with sublinear regret.

\subsection{Greedy}
Going the other way, a greedy algorithm will always select the arm with the highest empirical mean.
Here, all arms are tried $N$ initial times, and the empirical means are used to select the best arm.
Afterwards, the arm with the highest empirical mean is selected for all remaining turns.
The arm-selection procedure is listed in \cref{alg:greedy}, where $\hat{\mu}_i$ is the empirical mean of arm $i$.
\begin{algorithm}
    \caption{Greedy arm selection}
    \label{alg:greedy}
    \begin{algorithmic}
        \If{$t \leq Nk$}
        \State \Return $(t \mod k) + 1$
        \Else
        \State \Return $\argmax_{i=1,\dots,k} \hat{\mu}_i$
        \EndIf
    \end{algorithmic}
\end{algorithm}

With greedy selection, the expected regret is clearly still linear in the horizon, as there is a non-zero probability of selecting the wrong arm.
Still, there is a chance of achieving zero regret and the constant factor is reduced compared to random selection.
To improve hereupon, it is necessary to occasionally explore other arms, which leads into the epsilon-greedy algorithm.


\subsection{Epsilon-greedy}
The problem with the greedy algorithm is that it may be unlucky and not discover the best arm in the initial exploration phase.
To mitigate this, the epsilon-greedy algorithm may be used.
In this algorithm, the presumed best arm is pulled with probability $1-\epsilon$, while in the other $\epsilon$ proportion of the turns, an arm is pulled uniformly at random.
This ensures convergence to correct exploitation as the horizon increases, and it will generally reduce the regret.

Still, with a constant $\epsilon$, a constant proportion of the turns will be spent exploring, keeping the regret necessarily linear in the horizon.
Choosing $\epsilon$ is a trade-off between exploration and exploitation and can significantly affect the regret.

\begin{algorithm}
    \caption{Epsilon-greedy arm selection}
    \label{alg:eps_greedy}
    \begin{algorithmic}
        \If{$t \leq Nk$}
        \State \Return $(t \mod k) + 1$
        \Else
        \State Sample $u$ from $[0,1)$ uniformly
        \If{$u < \epsilon$}
        \State Sample $i$ from $\{1,\dots, k\}$ uniformly
        \State \Return $i$
        \Else
        \State \Return $\argmax_{i=1,\dots,k} \hat{\mu}_i$
        \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}


\subsubsection{Epsilon-decay}
To remedy the linear term in the regret, modifications to the epsilon-greedy algorithm have been proposed wherein $\epsilon$ is a function of the number of trials, $t$.
Specifically, in order to achieve sublinear regret, it is necessary to decay $\epsilon$ towards zero.
Decreasing $\epsilon$ over time makes intuitive sense; exploration is more crucial in the beginning stages of the algorithm, whereas exploitation is more important when the agent has more reliable estimates of the reward means.
For example, one popular strategy is to set $\epsilon \sim 1/t$, which has been shown to achieve logarithmic regret \cite{auer2002}.
It is worth noting, however, that the optimal decay rate depends on the specific problem instance, and achieving logarithmic regret can be challenging in practice \cite{bubeck2012}.

\subsection{UCB}
The upper confidence bound (UCB) algorithm is a more sophisticated algorithm based on estimating an upper bound for the mean of each arm.
One always chooses the arm whose upper confidence bound is highest, a principle known as \enquote{optimism in the face of uncertainty}.
This should make sense, as if the wrong arm appears best, it will be pulled more often and the empirical mean will be corrected, while the true best arm with its larger bound will eventually become highest and so pulled.
When exploiting the actual best arm, the agent can trust it to be the best, as the confidence bound will remain above those of all the other arms.

Assuming rewards in $[0,1]$ and using Hoeffding's inequality, one has
\begin{equation}
    p
    = P \left(\mu_i > \hat{\mu_i} + \text{UCB}_i \right)
    \leq \exp \left(-2N_i \text{UCB}_i^2 \right),
\end{equation}
where $\text{UCB}_i$ is the upper confidence bound for arm $i$ and $N$ is the number of times arm $i$ has been pulled.
Solving for $\text{UCB}_i$ gives
\begin{equation}
    \text{UCB}_i = \sqrt{\frac{-\ln p}{2N_i}},
\end{equation}
and letting $p = t^{-4}$ gives
\begin{equation}
    \text{UCB}_i = \sqrt{\frac{2 \ln t}{N_i}},
\end{equation}
which is a common choice for the upper confidence bound, leading to the UCB1-algorithm.
In \cite{auer2002}, it was shown that this algorithm achieves $O(\ln T)$ regret.
Regardless of the assumptions made, the procedure follows as in \cref{alg:ucb}.
Many variants of the algorithm exist; different assumptions about the distributions change the confidence bounds.
While the choice of $p$ is arbitrary, it is less of nuisance than the choice of $\epsilon$ in the epsilon-greedy algorithm, with specific choices of $p$, such as the UCB1-algorithm, being well-studied and known to perform well, achieving logarithmic regret.

\begin{algorithm}
    \caption{UCB arm selection}
    \label{alg:ucb}
    \begin{algorithmic}
        \If{$t \leq k$}
        \State \Return $t$
        \Else
        \State \Return $\argmax_{i=1,\dots,k} \hat{\mu}_i + \text{UCB}_i$
        \EndIf
    \end{algorithmic}
\end{algorithm}




\subsection{Bayesian: Thompson sampling}
Thompson sampling is a Bayesian approach to the multi-armed bandit problem, being the original approach to the problem \cite{thompson1933}.
This method is noteworthy for its ability to incorporate Bayesian modelling concepts into the fundamentally frequentist problem of multi-armed banditry.
The idea is to sample from the posterior distribution of the means of the arms and pull the arm with the highest sample.

It was first in 2012 that Thompson sampling was proven optimal for Bernoulli rewards \cite{kaufmann2012} with uniform priors.
Also for Gaussian rewards, it was proven optimal \cite{honda2014} with uniform priors.
Notably, the Jeffreys prior was shown to be inadequate in achieving optimal regret, highlighting the importance of the prior selection.

\begin{algorithm}
    \caption{Thompson sampling arm selection}
    \label{alg:thompson}
    \begin{algorithmic}
        \For {$i=1,\dots,k$}
        \State Sample $\theta_i$ from $P(\mu_i | \{r_i^{(1)}, \dots, r_i^{(N_i(t))}\})$
        \EndFor
        \State \Return $\argmax_{i=1,\dots,k} \theta_i$
        \State Update posterior for arm $i$ with reward $r_i$
    \end{algorithmic}
\end{algorithm}

One of the key advantages of Thompson sampling is that it can natively incorporate prior knowledge about the arms, whereas doing so with the above methods would require some sort of ad-hoc manipulation of the recorded rewards and arm pulls.
Furthermore, empirical results indicate it achieving lower regrets than the other algorithms \cite{kaufmann2012}.
Still, Thompson sampling is not without its drawbacks.
The algorithm can be computationally expensive, as it requires sampling from the posterior distribution for each arm at each time step.
Even with conjugate priors, the computational costs of sampling will be higher than the simple computations required by UCB and epsilon-greedy algorithms.

\section{Simulations?}