\chapter{Results}
\label{chap:simulations}

This chapter presents empirical evaluations of the quantum upper bound confidence bound algorithm's performance as it is described in~\cref{chap:qbandits} and its original proposal paper~\autocite{wan2022}.
Despite the algorithm's theoretical guarantees, promising asymptomatically lower regret bounds than any classical algorithm can achieve, how it performs in practice has not been extensively studied.
In~\autocite{wan2022}, some simulations were done to show its supreme performance versus the classical UCB algorithm, but only for certain fixed bandit instances.
Here, the algorithm is compared also with the overall better Thompson sampling algorithm, and by considering more bandit instances and Bayesian regrets, it is shown that the algorithm is only superior in some cases.

Furthermore, in \cref{sec:sim_rl}, modern reinforcement learning algorithms are tested on the bandit problem, including a novel quantum neural network policy agent presented in~\autocite{jerbi2021a}.
Applying such general algorithms to the bandit problem is not a common approach, but it is interesting to see how they perform and compare to the more problem-specific algorithms.

All algorithms were implemented in Python 3~\autocite{python} with quantum computing support provided by IBM's Qiskit library~\autocite{qiskit} for the QUCB algorithm and PennyLane for the quantum neural network policy agent.
The experiments were run on an external computation server with an Intel Xeon E5-2690 v4 CPU and 768 GB of RAM, permitting 28 concurrent simulations.
Approximately 25,000 CPU hours were used.
The source code for the simulations presented here is available on GitHub\footnote{
    Cf. \url{https://www.github.com/boyesjo/tma4900/}.
}.



\subimport{}{bandits}
\clearpage


% \subimport{}{bayesian}
% \clearpage

\subimport{}{rl}

\clearpage
\section{Interpretation}
% \section{Implications}
% \section{Discussion}
% \section{Significance}
% \section{Takeaways}
It is evident that the QUCB algorithm indeed has the potential to outperform classical algorithms in the bandit problem.
As seen in \cref{fig:big2,fig:high_prob}, the algorithm does not only outperform the classical UCB algorithm, but for these particular instances, also the better Thompson sampling algorithm.
However, as seen in \cref{fig:low_prob_fix,fig:random}, settings wherein quantum advantage is not achieved are also present.
While still performing better than classical UCB, the algorithm is outperformed by Thompson sampling for these \enquote{easier} bandits.

Reinforcement learning algorithms do not seem to be well suited for the bandit problem.
They do offer great power and flexibility, but only for problems correctly designed for them.
The bandit problem, even with some help in explicit observations of the means, do not seem to work well with these algorithms.
A quantum neural network policy could in principle inherit some QUCB advantages, but this would require a different, specialised data inputting scheme to allow direct input of the quantum states representing rewards.
As it stands, the simpler, but specialised bandit algorithms appear to be the best choice for the bandit problem.