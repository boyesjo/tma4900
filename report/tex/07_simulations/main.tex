\chapter{Results}
\label{chap:simulations}

This chapter presents empirical evaluations of the quantum upper bound confidence bound algorithm's performance as it is described in~\cref{chap:qbandits} and its original proposal paper~\autocite{wan2022}.
Despite the algorithm's theoretical guarantees, promising asymptomatically lower regret bounds than any classical algorithm can achieve, how it performs in practice has not been extensively studied.
In~\autocite{wan2022}, some simulations were done to show its supreme performance versus the classical UCB algorithm, but only for certain fixed bandit instances.
Here, the algorithm is compared also with the overall better Thompson sampling algorithm, and by considering more bandit instances and Bayesian regrets, it is shown that the algorithm is only superior in some cases.

Furthermore, in \cref{sec:sim_rl}, modern reinforcement learning algorithms are tested on the bandit problem, including a novel quantum neural network policy agent presented in~\autocite{jerbi2021a}.
Applying such general algorithms to the bandit problem is not a common approach, but it is interesting to see how they perform and compare to the more problem-specific algorithms.

All algorithms were implemented in Python 3~\autocite{python} with quantum computing support provided by IBM's Qiskit library~\autocite{qiskit} for the QUCB algorithm and PennyLane for the quantum neural network policy agent.
The experiments were run on an external computation server with an Intel Xeon E5-2690 v4 CPU and 768 GB of RAM, permitting 28 concurrent simulations.
Approximately 25,000 CPU hours were used.
The source code for the simulations presented here is available on GitHub\footnote{
    Cf. \url{https://www.github.com/boyesjo/tma4900/}.
}.

\clearpage


\subimport{}{bandits}
\clearpage


% \subimport{}{bayesian}
% \clearpage

\subimport{}{rl}

\clearpage
% \section{Interpretation}
% \section{Implications}
% \section{Significance}
% \section{Takeaways}
\section{Discussion}
In the above, the potential superiority of the quantum upper confidence bound algorithm was explored in solving the stochastic multi-armed bandit problem.
Specifically, the QUCB algorithm was tested against its classical ancestor, the UCB algorithm, and the oftentimes best classical algorithm, namely Thompson sampling.
These tests included static bandit instances where the means were fixed, and dynamic bandit instances the means were drawn from a prior distribution.
Additionally, state-of-the-art reinforcement learning algorithms were tested on the bandit problem, where also a quantum policy agent was included.

From the experiments, it is evident that the QUCB algorithm indeed has the potential to outperform classical algorithms in the bandit problem.
As seen in \cref{fig:big2,fig:high_prob}, the algorithm does not only outperform the classical UCB algorithm, but for these particular instances, also the better Thompson sampling algorithm.
However, as seen in \cref{fig:low_prob_fix,fig:random}, settings wherein quantum advantage is not achieved also arise.
While still performing better than classical UCB, the algorithm is outperformed by Thompson sampling for these \enquote{easier} scenarios.

Most visibly in \cref{fig:random}, the QUCB algorithm can be very inefficient in the early turns of a bandit problem, after which it may be too late for its asymptotic advantages to kick in.
With easy instances, such as the uniform prior in \cref{fig:random}, arm means are likely to be relatively far apart.
Accordingly, arms can with high probability be eliminated early.
The QUCB algorithm, however, does not take proper advantage thereof, and instead proceeds with its conservative strategy of producing quantum supreme estimates.

\Cref{fig:histograms_random} indicate that the QUCB regret distribution is more concentrated around the mean and is less affected by the bandit instance, but the regret curves in \cref{fig:random,fig:random2,fig:random3} show that it does actually matter which bandit instance is used.
It is likely the asymptomatic properties of the QUCB algorithm, namely the logarithmic upper bound, that enables its regret to flatten out as the number of rounds increases, achieving on occasion better final regrets at the time horizons that were attempted.
For truly great time horizons, it may be that the QUCB algorithm would always outperform the classical algorithms.

As briefly covered in the opening of this chapter, what can be considered the hyperparameter of the QUCB algorithm, that is its confidence level $\delta$ and the coefficient $C_1$, were not tuned or investigated in these experiments.
The confidence level $\delta$ determines at which probability the upper bound is guaranteed to hold, and should according to the logarithmic bound presented in \cref{sec:wan_proof} be set according to the time horizon $T$.
It was seen in the original paper and some preliminary testing here that fixing this value to something like $\delta = 0.01$ yields good results, at least for the instance in \cref{fig:big2}.
This may be a good value for all instances, but that is in no way certain, and it is as such something which could warrant deeper investigation.
In a similar manner, the coefficient $C_1$ was set rather arbitrarily to $2$.
It is not clear whether this is a good value, or if it should be set differently for different instances.
Overall, there may be better ways to tune the algorithm and extract more performance from it.
Still, changing these hyperparameters would not change the general properties of the algorithm, particularly its flat regret curves with an initial potentially costly learning period, and its tendencies that were observed here should likely remain.

In contrast, reinforcement learning algorithms did not seem to be well suited for solving the bandit problem.
They do offer great power and flexibility, but only for problems correctly designed for them.
The bandit problem, even with some help in explicit observations of the means, do not seem to work well with these algorithms.
A quantum neural network policy could in principle inherit some QUCB advantages, but this would require a different, specialised data inputting scheme to allow direct input of the quantum states representing rewards.
As it stands, the simpler, but specialised bandit algorithms appear to be the best choice for the bandit problem.