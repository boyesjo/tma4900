\chapter{Results}
\label{chap:simulations}

This chapter presents empirical evaluations of the quantum upper bound confidence bound algorithm's performance as it is described in~\cref{chap:qbandits} and its original proposal paper~\autocite{wan2022}.
Despite the algorithm's theoretical guarantees, promising asymptomatically lower regret bounds than any classical algorithm can achieve, how it performs in practice has not been extensively studied.
In~\autocite{wan2022}, simulations were done to show its supreme performance versus the classical UCB algorithm for certain fixed bandit instances.
Here, the algorithm is compared also with the overall better Thompson sampling algorithm, and by considering more bandit instances and Bayesian regrets, it is shown that the algorithm is superior to the classical algorithms for only a certain subset of bandit instances.

Furthermore, in \cref{sec:sim_rl}, modern reinforcement learning algorithms are tested on the bandit problem, including a novel quantum neural network policy agent presented in~\autocite{jerbi2021a}.
Applying such general algorithms to the bandit problem is not a common approach, but it is interesting to see how they perform and compare to the more problem-specific algorithms.

All algorithms were implemented in Python 3 (v3.10, v3.11)~\autocite{python} with quantum computing support provided by IBM's Qiskit library (v0.41)~\autocite{qiskit} for the QUCB algorithm and PennyLane (v0.29)~\autocite{pennylane} for the quantum neural network policy agent.
The experiments were executed on an external computation server with an Intel Xeon E5-2690 v4 CPU and 768 GB of RAM, permitting 28 concurrent simulations.
Approximately 25,000 CPU hours were used.
The source code for the simulations presented here is available on GitHub\footnote{
    Cf. \url{https://www.github.com/boyesjo/tma4900/code/}.
}.

\clearpage


\subimport{}{bandits}
\clearpage


% \subimport{}{bayesian}
% \clearpage

\subimport{}{rl}

\clearpage
% \section{Interpretation}
% \section{Implications}
% \section{Significance}
% \section{Takeaways}
\section{Discussion}
In the above, the potential superiority of the quantum upper confidence bound algorithm was explored in solving the stochastic multi-armed bandit problem.
Specifically, the QUCB algorithm was tested against its classical ancestor, the UCB algorithm, and the oftentimes best classical algorithm, namely Thompson sampling.
These tests included static bandit instances where the means were fixed, and dynamic bandit instances the means were drawn from a prior distribution, from which Bayesian regrets were studied.
Additionally, state-of-the-art reinforcement learning algorithms were tested on the bandit problem, where also a quantum policy agent was included.

From the experiments, it is evident that the QUCB algorithm indeed has the potential to outperform classical algorithms in the bandit problem.
As seen in \cref{fig:big2,fig:high_prob}, the algorithm does not only outperform the classical UCB algorithm, but for these particular instances, also the better Thompson sampling algorithm.
However, as seen in \cref{fig:low_prob_fix,fig:random}, settings wherein quantum advantage is not achieved also arise.
While still performing better than classical UCB, the algorithm is outperformed by Thompson sampling for these \enquote{easier} scenarios.

Most visibly in \cref{fig:random}, the QUCB algorithm can be very inefficient in the early turns of a bandit problem, after which it may be too late for its asymptotic advantages to be realised.
With easy instances, such as the uniform prior in \cref{fig:random}, arm means are customarily relatively far apart.
Accordingly, arms can be eliminated early with a high probability of the elimination being correct.
The QUCB algorithm, however, does not take proper advantage thereof, and instead proceeds with its conservative strategy of producing quantum supreme estimates.

\Cref{fig:histograms_random} indicate that the QUCB regret distribution is more concentrated around the mean and is less affected by the actual bandit instance, but the regret curves in \cref{fig:random,fig:random2,fig:random3} show that it does positively matter which bandit instance is at hand.
It is likely the asymptomatic regret properties of the QUCB algorithm, namely the logarithmic upper bound, that enables its regret to flatten out as the number of rounds increases, achieving on occasion better final regrets at the time horizons that were attempted.
For truly great time horizons, it may be the case that the QUCB algorithm would always outperform the classical algorithms.

As briefly covered in the opening of this chapter, what can be considered the hyperparameter of the QUCB algorithm, that is its confidence level $\delta$ and the coefficient $C_1$, were not tuned or investigated in these experiments.
The confidence level $\delta$ determines at which probability the upper bound, internally used by the algorithm to decide which arm to pull, is guaranteed to hold, and should according to the logarithmic bound presented in \cref{sec:wan_proof} be set according to the inverse time horizon $1/T$.
It was seen in the original paper and some preliminary testing for this thesis that fixing this value to a much higher $\delta = 0.01$ yields good results, at least for the instance in \cref{fig:big2}.
This may be a good value for all instances, but its certainty remains obscure, and it is as such something which could warrant deeper investigation.
In a similar manner, the coefficient $C_1$ was arbitrarily set to $2$.
It is not clear whether this is a good value, or if it should be set differently for different instances.
Overall, there may be better ways to tune the algorithm and extract lower regrets
Still, changing these hyperparameters would not change the fundamental properties of the algorithm, particularly its flatter regret curves with an initial, potentially costly, learning period, and its tendencies that were observed here should likely remain.

In contrast to the QUCB algorithm, reinforcement learning algorithms did not seem to be well suited for solving the bandit problem.
They do offer great power and flexibility, but only for problems correctly designed for them.
The bandit problem, even with some help in explicit observations of the means, do not seem to work well with these algorithms.
A quantum neural network policy could in principle inherit some QUCB advantages, but this would require a different, specialised data inputting scheme to allow direct input of the quantum states representing rewards.
For it to be possible, another architecture would be needed, such as a quantum recurrent neural network.
As it stands, the simpler, but specialised bandit algorithms appear to be the best choice for the bandit problem.