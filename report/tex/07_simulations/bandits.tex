\section{Quantum bandits}
While the upper bound shown in \cref{sec:wan2022} implies QUCB1 advantage, it does not necessarily mean it is supreme in all cases or even on average.
It is therefore of much interest to test it on a variety of problems and compare its average performance to other algorithms â€” not only with the classical UCB1 to which it is compared in the original paper, but also with the more performant Thompson sampling algorithm.

For fixed bandit instances, 100 simulations were run for each algorithm, while for Bayesian regrets 1000 parallels were run.
To limit the scope and computational resources required, only Bernoulli rewards were considered, and in all cases but one only two arms.
How the algorithm handles more arms may be of some interest, but its main advantage is its better dependence on the time horizon $T$, and it handles multiple arms the same was as the classical UCB algorithm.

Across all bandit simulations, a time horizon of $T=250,000$ was used.
This is indeed a large number, but still merely a quarter of what was tested in~\autocite{wan2022}.
The quantum algorithm presents no immediate advantage for smaller time horizons, so  a time horizon of some substance must be chosen to see its effects.
Specifically, due to its exponentially long quantum Monte Carlo periods, the horizon must be large enough for the greater accuracy of the QMC estimates to outweigh the cost of the long periods.

As noted in~\autocite{wan2022}, setting the confidence parameter to a higher value than the theoretically desirable $1/T$ leads to better performance.
It was thus set to $0.01$ for all experiments.
Moreover, the constant $C_1$ which was only defined existentially was arbitrarily set to $2$ across all experiments, as it was not clear how to choose it in a principled manner, and this value seemed able to reproduce the results of~\autocite{wan2022}.

\subsection{Fixed arms}
\label{sec:sim_fixed_arms}
First, the algorithms are tested on fixed bandit instances with two arms.
The mean of the first arm is set to $0.5$ and the mean of the second arm is set to $0.505$, an instance also tested in~\autocite{wan2022}.
The results are shown in \cref{fig:big2}, agreeing with the original paper; QUCB greatly outperforms UCB.
However, Thompson sampling which was not considered in~\autocite{wan2022}, is not that far behind.

Note the jagged and periodically completely flat behaviour of the QUCB algorithm regret.
This is due to the long QMC periods in which its quantum advantage is gained; it must repeatedly pull the same arm for the QMC estimates to be produced.
When the algorithm pulls the optimal arm, the change in regret is obviously zero.
Additionally, these periods get longer as the algorithm progresses, as the QMC estimates become more accurate.
The exponential lengthening of the QMC periods assure that such non-smooth and piecewise linear behaviour is to be expected.
Because of these long periods, there is not as many possible trajectories as with the classical algorithms, so the jaggedness persists despite averaging over many simulations.

\begin{figure}
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Turn},
        ylabel={Regret},
        legend entries={UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
        largexnumbers,
    }
    \subimport{figs}{big2}
    \caption{QUCB1 regret for two arms, with mean 0.5 and 0.505.}
    \label{fig:big2}
\end{figure}

\subsubsection{Low and high probabilities}
Next, more extreme mean values of $0.01$ and $0.005$ with a reward gap of $0.005$ equal to the above, are tested.
\Cref{fig:low_prob_fix} contains the results for these simulations.
There, it can be seen that QUCB beats UCB thoroughly still, as both algorithms achieving similar regrets as in \cref{fig:big2}, but now Thompson sampling is clearly superior.


\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
        }
        \subimport{figs}{low_prob_fix}
        \caption{Two arms, means 0.01, 0.005.}
        \label{fig:low_prob_fix}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
        }
        \subimport{figs}{high_prob}
        \caption{Regret for two arms, means 0.99, 0.9905.}
        \label{fig:high_prob}
    \end{subfigure}
    \label{fig:low_high_prob}
    \caption{QUCB1 regret for especially high and low means.}
\end{figure}

At these extreme values, it seems the gap must shrink for QUCB to be able to outperform Thompson sampling.
This is shown in \cref{fig:high_prob}, where the gap is reduced to $0.0005$,
Even so, the QUCB advantage is not as pronounced as in \cref{fig:big2}.

\subsubsection{Four arms}
As a final test, the number of arms is increased to four.
The results are plotted in \cref{fig:four_arms}.
Only cases with two arms were tested in the original paper, so this is a new test, further validating the correctness of the algorithm and its implementation.
The results are similar to the two-arm case, with QUCB outperforming UCB.
At these particular reward means, Thompson sampling performs very similarly to QUCB.
In general, QUCB provides no advantages or noteworthy changes from UCB with respect to the number of arms, so its behaviour should be expected to be similar as UCB when the number of arms is increased.

\begin{figure}
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Turn},
        ylabel={Regret},
        legend entries={UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
        largexnumbers,
    }
    \subimport{figs}{four_arms}
    \caption{Regret for four arms, with mean 0.5, 0.51, 0.52 and 0.53.}
    \label{fig:four_arms}
\end{figure}

\clearpage
\subsection{Bayesian regret}
\label{sec:results_bayesian}
As described in \cref{sec:bayesian-optimality}, the Bayesian regret is the average regret over some set prior.
What priors to choose is a topic for discussion, which will not be covered deeply in this section nor thesis in general.
It may not be the most relevant measure for which to optimise policies, as explained in \cref{sec:optimality}.
In any case, it provides a measure of robustness, measuring the performance over a range of possible instances.

\subsubsection{Uniform prior}
First, the simple case of two arms, whose means are independently drawn from a uniform distribution on the interval $[0, 1]$ will be considered.
The Bayesian regret for this case is on display in \cref{fig:random}.
All three were run on 1,000 instances sampled from the prior.
The results shown are the averages over these instances.

It is obvious that the Thompson sampling algorithm performs best.
QUCB does outperform UCB, but not by as much as the previously considered cases and not in the first $\approx 30,000$ turns.

The lack of any quantum advantage is probably due to this prior generally yielding \enquote{easy} problems, where the optimal arm can be quickly identified.
In such cases, the initial overhead of the quantum algorithm appears not to be worth the effort.
QUCB with its current QMC implementation relies on a large number of initial samples to produce it supreme estimates, but with the uniform prior and only two arms, the expected difference of means is a third, so the optimal arm can often be identified after only a few turns with high certainty.
Nonetheless, after the rather wasteful inaugural period, QUCB does indeed produce a very flat regret curse; when looking at the log-log-plot in \cref{fig:random}, it may seem like the QUCB regret will be lower than even Thompson sampling for some great and admittedly unrealistic number of turns.



\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
            ymax = 150,
        }
        \subimport{figs}{uniform_prior.tex}
        \caption{Linear scale.}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
        }
        \subimport{figs}{uniform_prior_loglog.tex}
        \caption{Log-log scale.}
    \end{subfigure}
    \caption[
        Bayesian regret for two arms, independent and uniform priors.
    ]
    {
        Bayesian regret for two arms, independent and uniform priors.
        While Thompson sampling performs best, the log-log plot may indicate that QUCB could be better with a very large number of turns.
        Thompson sampling performs best, but it can appear as QUCB would be best at unreasonably great time horizons.
    }
    \label{fig:random}
\end{figure}


\subsubsection{Challenging prior}
One might argue that the uniform prior is too easy, and that the quantum advantage should be more apparent in a more challenging prior.
Firstly, for real-world Bernoulli bandits, there may be reasons to believe that the means would lie closer to the endpoints 0 and 1 than in the middle.
Secondly, for the problem to be interesting and warrant the use of clever quantum algorithms or even just any bandit theory, the means should be assumed close to each other.
Thus, to consider a more challenging prior, the following was chosen:
\begin{equation}
    \label{eq:challenging_prior}
    \begin{aligned}
        \mu_1               & \sim \text{B}(0.5, 0.5),                 \\
        \text{logit}(\mu_2) & \sim \text{N}(\text{logit}(\mu_1), 0.1),
    \end{aligned}
\end{equation}
where the logit is given by
\begin{equation}
    \label{eq:logit}
    \text{logit}(\mu) = \log(\mu/(1-\mu)).
\end{equation}
The logit is used to ensure that the means are in the interval $[0, 1]$.
It is particularly well-suited for interpreting the means of Bernoulli bandits due to its ability to capture meaningful differences between probabilities. For example, on the logit scale, the distance between 0.001 and 0.101 is much greater than that between 0.4 and 0.5.
Its symmetry around zero and coverage of the entire real number line allow for a comprehensive representation of the reward means.

Running a successful 1,000 simulations with this prior, the regrets visualised in \cref{fig:random2} were obtained.
At the final 250,000th turn, the QUCB and Thompson sampling algorithms have virtually equal regrets.
QUCB lags behind in the beginning, but its regret curve appears flatter towards the end, and it is likely that it would beat Thompson sampling in the long run.
The UCB algorithm, on the other hand, is as before clearly outperformed by both QUCB and Thompson sampling.

\subsubsection{More challenging prior}
As the above prior proved to still too easy to demonstrate quantum advantage, a third was chosen, catering even more to QUCB's strengths discovered in \cref{sec:sim_fixed_arms}, id est having close means and means centred around one half.
The prior was constructed similarly to the one in \cref{eq:challenging_prior}, but with different parameters, namely:
\begin{equation}
    \label{eq:more_challenging_prior}
    \begin{aligned}
        \mu_1               & \sim \text{B}(2, 2),                      \\
        \text{logit}(\mu_2) & \sim \text{N}(\text{logit}(\mu_1), 0.02),
    \end{aligned}
\end{equation}

This simulation was also run for 1,000 parallels.
As is seen in \cref{fig:random3}, QUCB is reclaims superiority, while Thompson sampling still clearly outperforms UCB.
It is clear that this prior produces so challenging instances, that the advantage over pure exploration is not as apparent as in the previous priors, but more akin to the first considered fixed instances in \cref{sec:sim_fixed_arms}, where the QUCB advantage was initially observed\footnote{Viz. \cref{fig:big2} and \cref{fig:high_prob}.}.


\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
            ymax = 400,
        }
        \subimport{figs}{challenging_prior.tex}
        \caption{
            Prior as in \cref{eq:challenging_prior}.
        }
        \label{fig:random2}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            largexnumbers,
        }
        \subimport{figs}{more_challenging_prior.tex}
        \caption{
            Prior as in \cref{eq:more_challenging_prior}.
        }
        \label{fig:random3}
    \end{subfigure}
    \caption[
        Bayesian regret for two arms, challenging priors.
    ]
    {
        Bayesian regret for two arms, challenging priors,
        The lower figure has the means closer to the $1/2$ and even higher correlation between the means, which is more favourable to QUCB.
    }
    \label{fig:random_challenging}
\end{figure}