\section{Algorithms for the bandit problem}
By producing better predictions of the rewards of the arms, the QUCB algorithm promises to outperform classical algorithms in the multi-armed bandit problem.
While the upper bound shown in \cref{sec:wan2022} implies QUCB advantage, it does not necessarily mean it is supreme in all cases or even on average.
It is therefore interesting to test it on a variety of problems and compare its average performance to other algorithms â€” not only with the classical UCB to which it is compared in the original paper, but also with the more performant Thompson sampling algorithm\footnote{Q.v. \cref{chap:bandits}.}.
The UCB algorithm and Thompson algorithms were implemented in a straight-forward manner per the descriptions in \cref{sec:ucb,sec:thompson}, relying primarily on the NumPy (v1.23)~\autocite{numpy} and SciPy (v1.10) libraries~\autocite{scipy}, while the (simulation of the) QUCB algorithm was implemented as in explained in \cref{sec:qucb-implementation}, with the Qiskit library (v0.41)~\autocite{qiskit} for quantum computing support and simulations of quantum algorithms.

Remark that it is specifically the UCB1 of~\autocite{auer2002} that is implemented and that the digit is here omitted from the notation for brevity.
Similarly, what here is simply called QUCB is in the original paper called $\text{QUCB}_1$.
The Thompson sampling algorithm that is implemented follows the original paper ~\autocite{thompson1933}, with its uniform prior and conjugate Beta posterior distributions.

For fixed bandit instances, 100 simulations were run for each algorithm, while for Bayesian regrets 1000 parallels were run.
Simulating quantum algorithms is computationally expensive, in general exponentially hard\footnotemark.
To emulate the logic of a quantum computer, the classical libraries for simulated quantum computing effectively perform the linear algebra operations as described in \cref{chap:qc} in the exponentially large vector spaces, perhaps with some clever optimisations when possible.

Due to this computational cost, much higher numbers of simulations were not feasible for the QUCB algorithm and the time horizons considered on the hardware dedicated to the project.
As such, there is undoubtedly some Monte Carlo error in the results, but it is not expected to be significant.
The differences between the performances are in many cases large enough to draw statistically significant conclusions, and the results are consistent with the original paper~\autocite{wan2022}.

\footnotetext{
    The exponential complexity of simulating quantum mechanics is after all what lead to the conception of quantum computing in the first place (q.v. \cref{ch:intro}).
}

To limit the scope and computational resources required, only Bernoulli rewards were considered, and in every case except one, only two arms were used.
How the algorithm handles more arms may be of some interest, but its principal advantage is its better dependence on the time horizon $T$, and it handles multiple arms the same was as the classical UCB algorithm, so there is no reason to expect any regret improvements as the number of arms increases.

The QUCB algorithm that was implemented could in principle handle any rewards distributions with bounded values, but doing so would increase the qubit counts and in turn the computational cost of the quantum Monte Carlo estimates of the reward means
Likewise, the QUCB$_2$ algorithm for bounded variance rewards could be tested on even more problems, but it is too would demand yet more computational resources to test on long enough horizons to observe quantum advantage.

Across all bandit simulations, a time horizon of $T=250,000$ was used.
This is indeed a large number, but still merely a quarter of what was tested in~\autocite{wan2022}.
The quantum algorithm presents no immediate advantage for smaller time horizons, so  a time horizon of substance must be chosen to see its effects in the simulations.
Specifically, due to its exponentially long quantum Monte Carlo periods, as presented in \cref{sec:wan2022}, the horizon must be large enough for the greater accuracy of the QMC estimates to outweigh the cost of the long periods.

As noted in~\autocite{wan2022}, setting the confidence parameter to a higher value than the theoretically desirable $1/T$ leads to better performance.
It was for this reason set to $0.01$ across all experiments in this chapter.
Moreover, the constant $C_1$, which is only defined existentially, was arbitrarily set to $2$ across all experiments, as it was not clear how to choose it in a principled manner, and this value seemed able to reproduce the results of~\autocite{wan2022}.

\clearpage

\subsection{Fixed arms}
\label{sec:sim_fixed_arms}
First, the algorithms are tested on fixed bandit instances with two arms.
The mean of the first arm is set to $0.5$ and the mean of the second arm is set to $0.505$, an instance also tested in~\autocite{wan2022}.
The results are shown in \cref{fig:big2}.
QUCB greatly outperforms UCB.
However, Thompson sampling which was not considered in~\autocite{wan2022}, is not that far behind.

It is worth acknowledging that the algorithms do not outperform the random, uninformed baseline by substantial amounts.
Especially UCB, which performs worst of the three proper methods, does accumulate about two thirds of the baseline regret.
This is indicative of the difficulty of the instance, wherein the arm means are closely set.

This instance was chosen as it was tested in~\autocite{wan2022}, and by replicating the results thence, it is easier to trust that the implementation of the quantum upper confidence bound algorithm is correct.
Due to the computational cost, a slightly reduced time horizon of only $T=250,000$ was used, instead of the original paper's $T=1,000,000$, but this is enough to see the same trends as were reported in the original paper, and particularly notice the quantum advantage of QUCB when compared to its classical counterpart in UCB.

Note the jagged and periodically completely flat behaviour of the QUCB algorithm regret.
The regret is recorded at every turn, so this is not due to poor plotting.
Instead, this is caused by the long quantum Monte Carlo periods in which its quantum advantage is gained; it must repeatedly pull the same arm for the QMC estimates to be produced, and these periods are exponentially long in the time horizon $T$.
When the algorithm pulls the optimal arm, the change in regret is obviously zero.
Additionally, these periods get longer as the algorithm progresses, as the QMC estimates become more accurate.
The exponential lengthening of the QMC periods assure that such non-smooth and piecewise linear behaviour is to be expected.
Because of these long periods, there is not as many possible trajectories as with the classical algorithms, so the jaggedness persists despite averaging over many simulations.
Due to this exponential lengthening, similar behaviour should appear in all experiments with fixed instances regardless of the horizon.
Such behaviour is not seen in the classical algorithms, as they can pull any arm at any time, so there are many more possible trajectories, and the regret curves are much smoother.

\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
    }
    \subimport{figs}{big2}
    \caption[
        Regrets for two Bernoulli arms, with means 0.5 and 0.505.
    ]{
        Regrets for two Bernoulli arms, with means 0.5 and 0.505.
    }
    \label{fig:big2}
\end{figure}


\clearpage

\subsubsection{Low and high probabilities}
Next, more extreme mean values of $0.01$ and $0.005$ with a reward gap of $0.005$ equal to the above, are tested.
In the original paper, only cases of $0.5$ and a nearby value for the second arm were considered, but it is interesting to see how the quantum algorithm performs on more extreme values, and to what degree it is the difference of means that matters or some other function of the means.

\Cref{fig:low_prob_fix} contains the results for these simulations.
There, it can be seen that QUCB beats UCB thoroughly still, as both algorithms achieve similar regrets as in \cref{fig:big2}.
But now Thompson sampling is clearly superior.
UCB seems barely able to learn in the horizon considered, and QUCB took a substantial regret hit in the beginning, before its long flat QMC period and pulling of the optimal arm allowed it to beat UCB.


\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
    }
    \subimport{figs}{low_prob_fix}
    \caption{Regrets for Bernoulli arms with means 0.01 and 0.005.}
    \label{fig:low_prob_fix}
\end{figure}

\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
    }
    \subimport{figs}{high_prob}
    \caption{Regrets for two Bernoulli arms with means 0.99 and 0.9905.}
    \label{fig:high_prob}
\end{figure}

At these extreme values, it seems the gap must shrink for QUCB to be able to outperform Thompson sampling.
This is shown in \cref{fig:high_prob}, where the gap is reduced to $0.0005$,
Even so, the QUCB advantage is not as pronounced as in \cref{fig:big2}.
Here, UCB is very close to the random, uninformed baseline, while the two other algorithms are able to learn.
Still, their regret curves are still rising in the horizon considered.
It is clear that this is a very difficult instance, and perhaps a greater time horizon could have shown the quantum advantage of QUCB more clearly.

In \cref{fig:high_prob}, the UCB algorithm is only marginally better than the random baseline, and it is likely the case that its regret is not significantly different from the baseline at this horizon, considering the limited amount of simulations performed here of only $100$.
Evidently, this is such a hard instance that the UCB algorithm is unable to learn in the horizon considered.

It appears as for the upper confidence bound algorithms, solely the difference between arm rewards means determines the performance.
For the Thompson sampling algorithm, however, the absolute values of the means are also important, such that maybe some other scale like the difference of logit or probit means could be more relevant.
This may be due to its inlaid knowledge of Bernoulli rewards, while the UCB and QUCB algorithm tested here only assumes bounded rewards, and they must thus be more conservative.

\clearpage

\subsubsection{Four arms}
As a final test, the number of arms is increased to four.
The results are plotted in \cref{fig:four_arms}.
Only cases with two arms were tested in the original paper, so this is a new test, further validating the correctness of the algorithm and its implementation.
The results are similar to the two-arm case, with QUCB outperforming UCB.
At these particular reward means, Thompson sampling performs very similarly to QUCB.


With the extra arms, the qubit count is increased.
Therefore, the computation price of simulation and testing such cases is higher.
For this reason, no further increases in arm counts were investigated and no deeper studies in the case of four arms were performed.
In general, QUCB provides no advantages or noteworthy changes from UCB with respect to the number of arms.
While the dependence of the regret on the number of arms can be of major concern for many applications, this is not within the scope of this thesis.
Anyway, there is little reason to expect QUCB to show any different behaviour than UCB in this regard, as it handles the arms in the same way as classical UCB does.
It may be that its better predictions could lead to better performance in some cases.
There is no a priori reason to expect this, however, and in the bound described in \cref{sec:wan_proof}, the regret is only bounded linearly in the number of arms, which is worse than the square root dependence of the regret on the number of arms for UCB.
This may be a small price to ask for the logarithmic horizon dependency, and a square root bound for both is likely achievable, but this is not within the scope of this thesis.
Overall, the arm count is not a major concern for the QUCB algorithm, and it is not investigated further here.

In the figure, it is clear that all three algorithms significantly surpass the random baseline.
It is then perhaps the case that the instance chosen for this experiment had too easily identifiable optimal arms, and that more interesting behaviour and clear quantum supremacy could be observed for a more difficult instance.

\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
    }
    \subimport{figs}{four_arms}
    \caption[
        Regrets for four Bernoulli arms.
    ]
    {
        Regrets for four Bernoulli arms with means 0.5, 0.51, 0.52 and 0.53.
    }
    \label{fig:four_arms}
\end{figure}

\clearpage
\subsection{Bayesian regret}
\label{sec:results_bayesian}
As described in \cref{sec:bayesian-optimality}, the Bayesian regret is the average regret over some prior.
What priors to choose is a topic for discussion, which will not be covered deeply in this section nor thesis in general.
It may not be the most relevant measure for which to optimise policies.
In any case, it provides a measure of robustness, measuring the performance over a range of possible instances.

The use of Bayesian regrets instead of the regret for some fixed bandit instances is rarely seen in the literature.
No such experiments were done in~\autocite{wan2022}, so what follows are new results.
The same three algorithms as in the previous section are tested, but now on a different set of bandit instances drawn from different prior distributions for the arms.
With Bernoulli rewards that are determined by their mean and just two arms, these priors are distributions over $[0, 1]^2$.


\subsubsection{Uniform prior}
First, the simple case of two arms, whose means are independently drawn from a uniform distribution on the interval $[0, 1]$ will be considered.
The Bayesian regret for this case is on display in \cref{fig:random}.
All three were run on 1,000 instances sampled from the prior.
Having randomness also in the bandit instances motivated the use of a larger number of simulations than in the previous experiments.
The results shown are the averages over these instances.
The distribution of the final regrets is considered last in this subsection.

It is obvious that the Thompson sampling algorithm performs best.
QUCB does outperform UCB, but not by as much as the previously considered cases and not in the first $\approx 30,000$ turns.

The lack of any quantum advantage is probably due to this prior generally yielding \enquote{easy} problems, where the optimal arm can be quickly identified.
In such cases, the initial overhead of the quantum algorithm appears not to be worth the effort.
QUCB with its current QMC implementation relies on a large number of initial samples to produce it supreme estimates, but with the uniform prior and only two arms, the expected difference of means is a third, so the optimal arm can often be identified after only a few turns with high certainty.
Nonetheless, after the rather wasteful inaugural period, QUCB does indeed produce a very flat regret curve; inspecting the log-log-plot in \cref{fig:random}, it may seem like the QUCB regret curve would lie below even Thompson sampling at some great and admittedly unrealistic number of turns, perhaps something around the order of $10^{9}$.



\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Kiloturn},
            ylabel={Regret},
            legend entries={Baseline, UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
            ymax = 150,
        }
        \subimport{figs}{uniform_prior.tex}
        \caption{Linear scale.}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{\textwidth}
        \centering
        \newcommand{\myoptions}{
            width=10cm,
            height=8cm,
            xlabel={Turn},
            ylabel={Regret},
            legend entries={Baseline, UCB, QUCB, Thompson},
            legend pos=north west,
            legend cell align=left,
            mystyle,
        }
        \subimport{figs}{uniform_prior_loglog.tex}
        \caption{Log-log scale.}
    \end{subfigure}
    \caption[
        Bayesian regret for two Bernoulli arms, uniform prior.
    ]
    {
        Bayesian regret for two arms with independent and uniform priors.
    }
    \label{fig:random}
\end{figure}

\clearpage


\subsubsection{Challenging prior}
One might argue that the uniform prior is too easy, and that the quantum advantage should be more apparent in a more challenging prior.
Firstly, for real-world Bernoulli bandits, there may be reasons to believe that the means would lie closer to the endpoints 0 and 1 than in the middle.
Secondly, for the problem to be interesting and warrant the use of clever quantum algorithms or even any bandit theory whatsoever, the means should be assumed close to each other.
With means that lie far apart, the optimal arm can be identified with high certainty after only a few turns.
Such problems are certainly also interesting, but it is not for these that the quantum algorithm is designed and for which it can be expected to provide benefits for.

\begin{figure}[p]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={$\mu_1$},
                ylabel={$\mu_2 - \mu_1$},
                colorbar,
                colormap/plasma,
                point meta min=0,
                point meta max=47.6,
                view={0}{90},
                width=10cm,
                height=8cm,
                enlargelimits=false,
                axis on top,
                yticklabel style={
                        /pgf/number format/fixed,
                        /pgf/number format/precision=3
                    },
                scaled y ticks=false,
            ]
            \addplot graphics [
                    xmin=0,
                    xmax=1,
                    ymin=-0.1,
                    ymax=0.1,
                ] {figs/priors/prior1};
        \end{axis}
    \end{tikzpicture}
    \caption[
        Challenging prior used in Bayesian regret experiment.
    ]
    {
        Challenging prior used in for Bayesian regret experiment, as defined in \cref{eq:challenging_prior}.
        Note that the density diverges at $(\mu_1, \mu_2) = (0, 0)$ and $(1, 1)$.
        Values above the colour bar maximum are therefore clipped.
    }
    \label{fig:prior1}
\end{figure}

Thus, to consider a more challenging prior, the following was chosen:
\begin{equation}
    \label{eq:challenging_prior}
    \begin{aligned}
        \mu_1               & \sim \text{B}(0.5, 0.5),                 \\
        \text{logit}(\mu_2) & \sim \text{N}(\text{logit}(\mu_1), 0.1),
    \end{aligned}
\end{equation}
where the logit function is defined as
\begin{equation}
    \label{eq:logit}
    \text{logit}(\mu) = \log(\mu/(1-\mu)).
\end{equation}
This prior is displayed in \cref{fig:prior1}.
The difference of the means is there plotted rather than the second mean, as the joint density of both arms would simply appear non-informatively as a thin diagonal line.

The logit is used to ensure that the means are in the interval $[0, 1]$.
It is particularly well-suited for interpreting the means of Bernoulli bandits due to its ability to capture meaningful differences between probabilities. For example, on the logit scale, the distance between 0.001 and 0.101 is much greater than that between 0.4 and 0.5.
Its symmetry around zero and coverage of the entire real number line allow for a comprehensive representation of the reward means.



Running a successful 1,000 simulations with this prior, the regrets visualised in \cref{fig:random2} were obtained.
At the ultimate 250,000th turn, the QUCB and Thompson sampling algorithms have virtually equal regrets.
QUCB lags behind Thompson sampling in the beginning, but its regret curve appears flatter towards the end, and it is likely that it would beat Thompson sampling in the long run.
The UCB algorithm, on the other hand, is as before clearly outperformed by both QUCB and Thompson sampling.

\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
        ymax = 400,
    }
    \subimport{figs}{challenging_prior.tex}
    \caption[
        Bayesian regret for two Bernoulli arms, challenging prior.
    ]
    {
        Bayesian regret for two Bernoulli arms with a challenging prior as in \cref{eq:challenging_prior}.
    }
    \label{fig:random2}
\end{figure}

\clearpage
\vspace{2cm}
\subsubsection{More challenging prior}
As the above prior proved to still too easy to demonstrate quantum advantage, a third was chosen, catering even more to QUCB's strengths discovered in \cref{sec:sim_fixed_arms}.
Based on the relative performances of QUCB and Thompson sampling in \cref{fig:big2} and \cref{fig:low_prob_fix}, the quantum algorithm seems to perform relatively better when the arm rewards means are centred around $1/2$.
Furthermore, as should have been made clear throughout this thesis, the quantum algorithm is deigned to be most useful when the arms are close to each other.
Accordingly, the even more challenging prior was constructed similarly to the above one in \cref{eq:challenging_prior} for the same reasons and for simplicity, but with different parameters to achieve the desired effect.
The resulting prior is
\begin{equation}
    \label{eq:more_challenging_prior}
    \begin{aligned}
        \mu_1               & \sim \text{B}(2, 2),                      \\
        \text{logit}(\mu_2) & \sim \text{N}(\text{logit}(\mu_1), 0.02),
    \end{aligned}
\end{equation}
as is rendered in \cref{fig:prior2}.
There it can be visually confirmed that the means are not only more likely to be close to $1/2$, but closer to each other as well â€” at least when $\mu_1$ is close to $1/2$.

\begin{figure}[p]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                width=10cm,
                height=8cm,
                enlargelimits=false,
                axis on top,
                xlabel={$\mu_1$},
                ylabel={$\mu_2 - \mu_1$},
                yticklabel style={
                        /pgf/number format/fixed,
                        /pgf/number format/precision=3
                    },
                scaled y ticks=false,
                colorbar,
                colormap/plasma,
                point meta min=0,
                point meta max=149.6,
                view={0}{90},
            ]
            \addplot graphics [
                    xmin=0,
                    xmax=1,
                    ymin=-0.02,
                    ymax=0.02,
                ] {figs/priors/prior2};
        \end{axis}
    \end{tikzpicture}
    \caption[
        More challenging prior used in Bayesian regret experiment.
    ]
    {
        More challenging prior used in for Bayesian regret experiment, as defined in \cref{eq:more_challenging_prior}.
    }
    \label{fig:prior2}
\end{figure}


This simulation was also conducted with 1,000 parallels from which the average regrets at each turn is graphed in \cref{fig:random3}.
There, it is seen that QUCB reclaims superiority, while Thompson sampling still clearly outperforms UCB.
It is clear that this prior produces so challenging instances, that the advantage over pure exploration is not as apparent as in the previous priors, but more akin to the first considered fixed instances in \cref{sec:sim_fixed_arms}, where the QUCB advantage was initially observed\footnote{Viz. \cref{fig:big2,fig:high_prob}.}.

Unlike the uniform prior in \cref{fig:random}, where the baseline is completely outclassed by the more sophisticated algorithms already in the first few turns, and the challenging prior in \cref{fig:random2}, where the baseline is still clearly subjugated by the three more advanced algorithms, the baseline here in \cref{fig:random3} is now in the same order of magnitude as the other algorithms.
This indicates that this prior is indeed more challenging than the previous ones, placing its difficulty more in line with the one considered in \cref{fig:big2}.


\begin{figure}[p]
    \centering
    \newcommand{\myoptions}{
        width=10cm,
        height=8cm,
        xlabel={Kiloturn},
        ylabel={Regret},
        legend entries={Baseline, UCB, QUCB, Thompson},
        legend pos=north west,
        legend cell align=left,
        mystyle,
    }
    \subimport{figs}{more_challenging_prior.tex}
    \caption[
        Bayesian regret for two Bernoulli arms, more challenging prior.
    ]
    {
        Bayesian regret for two Bernoulli arms with a more challenging prior as in \cref{eq:more_challenging_prior}.
    }
    \label{fig:random3}
\end{figure}

\clearpage

\subsubsection{Regret distributions}
With more simulations run, it is possible to garner more information about the performance of the algorithms.
In \cref{fig:histograms_random}, the regrets at the final turn for the three algorithms are shown for the three priors that were tested.
The distributions are clearly wide, but with 1,000 samples, the conclusions drawn in the previous sections are nevertheless significant.

For the uniform prior, Thompson sampling is unquestionably supreme, while for the prior of \cref{eq:challenging_prior}, QUCB and Thompson achieve a similar average regret, but with QUCB's distribution being more concentrated.
It seems that even the classical UCB algorithm has a better mode than QUCB, but with its heavy tail it is still outperformed by QUCB on average.
Across all three histograms, it appears as QUCB is more consistent in its performance, being less dependent on the random instances that are generated, while Thompson sampling, though better on many instances, suffers from outliers with very high regret.
QUCB seemingly produces equal regrets independent of the prior, while the two classical algorithms perform worse on the more challenging priors.
This could perhaps be explained with its logarithmic instance-independent regret bound as shown in \cref{sec:wan_proof}.
Classically, such bounds are at best proportional to the square root of the horizon.
With its early and expensive exploration period, the QUCB algorithm is able to identify the best arm within the horizons considered here, seemingly independent of the difficulty of the instance.

The classical UCB algorithm appears to have even more spread in its regret distribution than Thompson sampling, and simply worse performance overall.
This is to be expected, as it does not achieve the same guarantees as Thompson sampling, as discussed in \cref{chap:bandits}.



\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                ybar,
                xlabel={Regret},
                ylabel={No. of occurrences},
                xmin = 0,
                ymin = 0,
                % xtick pos = bottom,
                xtick style={draw=none},
                grid = major,
                group style={
                        group size=1 by 3,
                        vertical sep=2.5cm,
                    },
            ]
            \nextgroupplot[
                title={Uniform prior},
                width=10 cm,
                height=5 cm,
                xmax = 600,
                % legend style={at={(0.5,-0.2)},anchor=north},
            ]
            \pgfplotstableread[col sep=comma]{../code/wan/results/random/final_turn_regerts.dat}\datatable
            \addplot+ [hist={bins=60}, red, opacity=0.5]
            table [y index=2, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, blue, opacity=0.5]
            table [y index=0, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, green, opacity=0.5]
            table [y index=1, col sep=comma]
            \datatable;

            \legend{UCB, QUCB, Thompson};

            \nextgroupplot[
                title={Prior as in \cref{eq:challenging_prior}},
                width=10 cm,
                height=5 cm,
                xmax = 600,
                % legend style={at={(0.5,-0.2)},anchor=north},
            ]
            \pgfplotstableread[col sep=comma]{../code/wan/results/random2/final_turn_regerts.dat}\datatable
            \addplot+ [hist={bins=60}, red, opacity=0.5]
            table [y index=2, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, blue, opacity=0.5]
            table [y index=0, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, green, opacity=0.5]
            table [y index=1, col sep=comma]
            \datatable;

            \legend{UCB, QUCB, Thompson};

            \nextgroupplot[
                title={Prior as in \cref{eq:more_challenging_prior}},
                width=10 cm,
                height=5 cm,
                xmax = 600,
                % legend style={at={(0.5,-0.2)},anchor=north},
            ]
            \pgfplotstableread[col sep=comma]{../code/wan/results/random3/final_turn_regerts.dat}\datatable
            \addplot+ [hist={bins=60}, red, opacity=0.5]
            table [y index=2, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, blue, opacity=0.5]
            table [y index=0, col sep=comma]
            \datatable;

            \addplot+ [hist={bins=60}, green, opacity=0.5]
            table [y index=1, col sep=comma]
            \datatable;

            \legend{UCB, QUCB, Thompson};

        \end{groupplot}
    \end{tikzpicture}

    \caption[
        Histograms of final turn regret.
    ]
    {
        Histograms of final turn regret for the three priors considered.
        Note that the rightmost bins are open-ended; they include all values greater than the highest regret value indicated.
        This was done to keep the interesting parts of the histograms at a reasonable scale.
    }
    \label{fig:histograms_random}
\end{figure}
