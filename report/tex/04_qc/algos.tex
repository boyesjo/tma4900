\section{Quantum algorithms}
\subsection{Computational complexity and quantum supremacy}
Quantum computers allure stems from their ability to solve certain problems more efficiently, exponentially so, than classical computers can.
Most (in-) famous of these problems are integer factorisation and discrete logarithms, which could be used to break effectively all public-key cryptography, but there are still several more problems for which quantum computers are believed to offer great advantages.
It is proven that the class of problems quantum computers can solve in polynomial time (with high probability), $\BQP$, contains the complexity class $\P$~\autocite{nielsen2012} and $\BPP$, the class of problems that can be solved in polynomial time with a classical probabilistic algorithm~\autocite{nielsen2012}.
This follows from the fact that quantum computers can efficiently run any efficient classical algorithm and reproduce any classical randomness by performing quantum measurements.
To produce a sample from a desired probability distribution, a corresponding superposition can be prepared and measured.

The Deutsch-Jozsa algorithm~\autocite{deutsch1992}, while of no practical use, is proved to be exponentially faster than any deterministic classical algorithm\footnotemark~\autocite{simon1994}.
Since quantum computers can solve problems like integer factorisation and discrete logarithms efficiently, it is believed that $\BQP$ is strictly greater than $\P$ by containing elements of $\NP \setminus \P$, but as whether $\P = \NP$ is unknown, these problems could turn out actually to be in $\P$.
In a similar vein, $\NP$-complete problems are believed to lie outside $\BQP$, such neither $\BQP$ nor $\NP$ contains the other, but this is again unproven~\autocite{aaronson2010}.

\footnotetext{
    The provable exponential speed-up does not prove $\P \neq \BQP$, as the algorithm requires an oracle~\autocite{johansson2017}.
    Oracle separations, a weaker notion than true inequality, have been found not only between $\P$ and $\BQP$, but also between $\BQP$ and $\BPP$ with Simon's algorithm~\autocite{simon1994}.
}

The complexity hierarchy can be expressed thus:
\begin{center}
    \begin{tikzpicture}

        \node at (0,0) (p) {\P};
        \node at (2,-1) (bpp) {\BPP};
        \node at (4,-1) (bqp) {\BQP};
        \node at (6,0) (pspace) {\PSPACE};
        \node at (3,1) (np) {\NP};

        \draw[->] (p) -- (bpp); % node[midway,above] {$\subseteq$};
        \draw[->] (bpp) -- (bqp); % node[midway,above] {$\subseteq$};
        \draw[->] (bqp) -- (pspace); % node[midway,above] {$\subseteq$};

        \draw[->] (p) -- (np); % node[midway,above] {$\subseteq$};;
        \draw[->] (np) -- (pspace); % node[midway,above] {$\subseteq$};;

        % \draw[<->, dashed] (np) -- (bqp) node[midway,above] {?};
        % \draw[<->, dashed] (np) -- (bpp) node[midway,above] {?};

    \end{tikzpicture}

\end{center}
Other than $\P \stackrel{?}{=} \BPP$, it is widely conjectured that the containments are strict, but this is not proven.
As disproving $\P = \PSPACE$ remains elusive, proving that $\BQP$ is strictly greater than $\P$ must be even harder.
It is therefore hard to say anything definite about quantum supremacy and being able to solve problems exponentially faster than can be done classically.

The exponential speed-ups that are discovered have not come easily.
Although the states spaces are exponentially large, with only a limited set of operations available, states can not be created and manipulated arbitrarily without exponentially many steps.
What is more, recapturing all the information in a general state can not be done without exponentially many measurements, negating any exponential gains.
The problem must have some structure to be exploited for a speed-up to be possible.
Quantum computers have only been shown to solve certain problems exponentially faster than classical computers, and finding the algorithms to do so is not trivial.
Shor's algorithm has time complexity $O((\log N)^3)$, while the most efficient known classical algorithm, the general number field sieve, is \enquote{sub-exponential} with a time complexity on the form $\Omega(k^{\frac{1}{3}}\log^{2/3}k)$, where $k=O(2^N)$~\autocite{dervovic2018}.
To solve linear systems, there is the HHL algorithm with time complexity $O(\log(N)\kappa^2)$, where $\kappa$ is the condition number.
This is an exponential speed-up over the fastest known classical algorithm\footnotemark{}, which has time complexity $O(N \kappa)$.
Still, these are non-trivial algorithms, not yet usable in practice and that were not easily found.

\footnotetext{
    Given that the condition number does not grow exponentially.
    There are also difficulties in loading the data into the quantum computer and extracting the solution that could negate any exponential speed-up.
    Cf.~\autocite{aaronson2015}.
}

Polynomial speed-ups are more easily found.
For example, the Grover algorithm which is used to search for an element in an unsorted list has time complexity $O(\sqrt{N})$~\autocite{grover1996}.
Classically, this can not be done in less than $O(N)$ time.
It can be proven that the Grover algorithm is optimal~\autocite{zalka1999}, so for this problem, an exponential speed-up is impossible.
This algorithm and the more general amplitude amplification on which it builds solves very general problems and are often used subroutines to achieve quadratic speed-ups in other algorithms.
Being only a quadratic speed-up, it is not as impressive as the exponential speed-ups, and achieving quantum supremacy in that manner would require larger quantum computers than if the speed-up were exponential.

\subsection{Grover's algorithm}
The quantum search algorithm of Grover~\autocite{grover1996} is a quantum algorithm that finds an element in an unstructured list with high probability.
While such a problem necessarily requires $O(N)$ time in a classical setting, needing on average $N/2$ steps to find the element and in the worst case $N$, Grover's algorithm finds the element in $O(\sqrt{N})$ steps.
This is a quadratic speed-up.

Grover's algorithm is provably optimal; no quantum algorithm can perform such general, unstructured searches any faster~\autocite{zalka1999}.
This should not be surprising.
If an exponential speed-up were possible, Grover search could be used to find the solution to NP-hard problems fast.

For Grover's algorithm to work, assume there is a function $f$ that maps the index of an element to $1$ if it is the one desired and $0$ otherwise.
(Assume for now that there is only one such element.)
Then, one assumes access to a quantum oracle, $\mathcal{O}_f$
(effectively a black box subroutine) that implements $f$ thus:
\begin{equation}
    \label{eq:grover_oracle}
    \mathcal{O}_f \ket{x} = (-1)^{f(x)} \ket{x}.
\end{equation}
A single application of this oracle is not enough to find the desired element, as the square of the amplitude of the desired element remains unchanged.
The measurement probabilities are not immediately affected.
Central to Grover's algorithm is the idea of amplifying the amplitude of the desired element.
This is done by applying a sequence of operations that is repeated until the amplitude of the desired element is large enough for it to be most likely to be measured, while the other elements have their amplitudes reduced.

Let the state $\ket{w}$ which be the winner state, a state with amplitude $1$ for the desired element and $0$ for all others.
Then consider the state $\ket{s}$, which is a uniform superposition state, a state with equal amplitudes for all elements, trivially constructed by applying a Hadamard gate to each qubit in the canonical starting state $\ket{0}$.
Define the state $\ket{s'}$ by subtracting the projection of $\ket{w}$ onto $\ket{s}$ from $\ket{s}$:
\begin{equation}
    \label{eq:grover_s'}
    \ket{s'} = \ket{s} - \bra{w}\ket{s}\ket{w}.
\end{equation}
These two orthogonal states form a basis of a two-dimensional subspace of the greater Hilbert space.
This permits a perspicuous visualisation of the algorithm, as in \cref{fig:grover}.
The uniform superposition state $\ket{s}$ serves as the starting point for the algorithm, and is expressible as
\begin{equation}
    \ket{s} = \cos(\theta)\ket{s'} + \sin{(\theta)}\ket{w},
\end{equation}
where $\theta=\arcsin\bra{s}\ket{w}=\arcsin{}(1/\sqrt{N})$.

Applying the oracle on $\ket{s}$ leaves its $\ket{s'}$ component unchanged, but flips the sign of the $\ket{w}$ component.
This results in the state $\ket{\psi} = \cos(-\theta)\ket{s'} + \sin{(-\theta)}\ket{s'}$, which can be seen as reflection of $\ket{s}$ in the $\ket{s'}$ direction.


Next, the state $\ket{\psi}$ is reflected about the initial $\ket{s}$ state, resulting in the state $\ket{\psi'} = \cos(3\theta)\ket{s'} + \sin(3\theta)\ket{w}$.
Reflection thus is achieved by the diffusion operator
\begin{equation}
    D=H^{\otimes n} S_0 (H^{\otimes n})^{-1} = H^{\otimes n} S_0 H^{\otimes n},
\end{equation}
where $S_0= 2\ketbra{0}{0} - I$ is the reflection operator about the $\ket{0}$ state, that is an operator that flips the sign of all but the $\ket{0}$ component.

The product of the oracle and the diffusion operator defines the Grover operator, which is simply applied until the amplitude of the  $\ket{w}$ is sufficiently amplified.
After $k$ iterations, the state is $\ket{\psi_k} = \cos((2k+1)\theta)\ket{s'} + \sin((2k+1)\theta)\ket{w}$.
Measuring the correct state has probability $\sin^2((2k+1)\theta)$.
Therefore, $k \approx {\pi}/{4\theta}$ iterations should be completed.
Assuming large $N$, for a short list would not warrant the use of Grover's algorithm, $\theta = \arcsin{({1}/{\sqrt{N}})} \approx 1/\sqrt{N}$, and so $k \approx \pi\sqrt{N}/4$.

For lists with more than a single desired element, a similar reasoning will lead to the same algorithm, but instead with $k\approx\pi/4 \cdot \sqrt{N/M}$, where $M$ is the amount of solutions to $f(x)=1$~\autocite{nielsen2012}.


\begin{figure}
    \centering
    \begin{tikzpicture}
        \node (O) at (3,3) {\textbf{\textsf{(a)}}};
        \def\ang{15}
        \coordinate (O) at (0,0);
        \coordinate (S) at (3,0);
        \coordinate (W) at (0,3);
        \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});

        % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
        \draw[->] (O) -- (W) node[above] {$\ket{w}$};
        % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

        \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
        \draw[->, blue] (O) -- (P) node[right] {$\ket{\psi}$ = $\ket{s}$};
        \pic[
            "$\theta$",
            draw=black,
            angle radius = 1cm,
            pic text options={
                    shift = {(0.8 cm, 0.1 cm)}
                }
        ] {angle = S--O--P};

        \begin{scope}[shift={(6, 0)}]
            \node (O) at (3,3) {\textbf{\textsf{(b)}}};
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            \draw[->, blue] (O) -- (P2) node[below] {$\ket{\psi}=\mathcal{O}_f\ket{s}$};
            \pic[
                "-$\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(0.8 cm, -0.1 cm)}
                    }
            ] {angle = P2--O--S};
            \pic[
                <-,
                draw=gray,
                angle radius = 2.5cm,
            ] {angle = P2--O--P};
        \end{scope}

        \begin{scope}[shift={(0, -6)}]
            \node (O) at (3,3) {\textbf{\textsf{(c)}}};
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});
            \coordinate (P3) at ({3*cos(3*\ang)}, {3*sin(3*\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            \draw[->] (O) -- (P2) node[below] {$\mathcal{O}_f\ket{s}$};
            \draw[->, blue] (O) -- (P3) node[right] {$\ket{\psi}=D\mathcal{O}_f\ket{s}$};
            \pic[
                "$2\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(0.6 cm, 0.4 cm)}
                    }
            ] {angle = P--O--P3};
            \pic[
                ->,
                draw=gray,
                angle radius = 2.5cm,
            ] {angle = P2--O--P3};
        \end{scope}


        \begin{scope}[shift={(6, -6)}]
            \node (O) at (3,3) {\textbf{\textsf{(d)}}};
            \def\ang{7}
            \coordinate (O) at (0,0);
            \coordinate (S) at (3,0);
            \coordinate (W) at (0,3);
            \coordinate (P) at ({3*cos(\ang)}, {3*sin(\ang)});
            % \coordinate (P2) at ({3*cos(-\ang)}, {3*sin(-\ang)});
            % \coordinate (P3) at ({3*cos(3*\ang)}, {3*sin(3*\ang)});
            \coordinate (P4) at ({3*cos(12*\ang)}, {3*sin(12*\ang)});

            % \draw[->] (o) -- (sp) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (W) node[above] {$\ket{w}$};
            % \draw[->, blue] (o) -- (psi) node[above] {$\ket{\psi}$ = $\ket{s}$};

            \draw[->] (O) -- (S) node[right] {$\ket{s'}$};
            \draw[->] (O) -- (P) node[above] {$\ket{s}$};
            % \draw[->] (O) -- (P2) node[right] {$\ket{\psi}=\mathcal{O}_f\ket{s}$};
            \draw[->, blue] (O) -- (P4) node[right] {$\ket{\psi}=G^k\ket{s}$};
            \pic[
                "$(2k+1)\theta$",
                draw=black,
                angle radius = 1cm,
                pic text options={
                        shift = {(1.2 cm, 0.4 cm)}
                    }
            ] {angle = S--O--P4};
        \end{scope}
    \end{tikzpicture}
    \caption[
        Grover's algorithm.
    ]
    {
        Grover's algorithm visualised.
        (a) The initial uniform superposition state $\ket{s}$ is prepared, which can be seen as a linear combination of $\ket{w}$ and $\ket{s'}$, forming an angle $\theta$ to the $s'$-axis.
        (b) The oracle $\mathcal{O}_f$ is applied to $\ket{s}$, flipping the sign of its $\ket{w}$ component, inverting the angle $\theta$.
        (c) The diffusion operator $D$ is applied, reflecting the state about the initial state and towards the goal, resulting in a state with an angle $3\theta$ to the $w$-axis.
        (d) After repeating the previous two steps a $k$ times, the angle is $2k+1\theta$, and if $k$ is chosen wisely, this means that the system is in a state close to the desired state $\ket{w}$, such that measuring the system will likely result in $\ket{w}$.
    }
    \label{fig:grover}
\end{figure}

\subsection{Amplitude amplification}
\label{sec:amplitude-amplification}
Amplitude amplification can be considered a generalisation of Grover's algorithm.
Instead of a single oracle, let the partitioning of the state space be given by a Hermitian projector $P$, whose image will be the space of states to amplify.
Then, for some initial state $\ket{\psi}$, it is decomposed into the orthogonal components
\begin{equation}
    \ket{\psi} = \sin(\theta)\ket{\psi_0} + \cos(\theta)\ket{\psi_1},
\end{equation}
where $\ket{\psi_1} = P\ket{\psi}$ and $\ket{\psi_0} = \ket{\psi} - \ket{\psi_1}$, effectively the projections onto the image and kernel of $P$.
Clearly, the angle $\theta$ is given by $\arcsin(|P\ket{\psi}|)$.

The Grover operator is now given by $G=-S_\psi S_P$ where
\begin{align}
    S_\psi & = I - 2\ket{\psi}\bra{\psi} \\
    S_P    & = I - 2P,
\end{align}
such that $S_\psi$ being analogue to the diffusion operator $D$ and $S_P$ being the oracle operator.

Following the same reasoning as in the previous section, the state after $k$ iterations is given by
\begin{equation}
    G^k \ket{\psi} = \sin((2k+1)\theta)\ket{\psi_1} + \cos((2k+1)\theta)\ket{\psi_0},
\end{equation}
meaning that $k \approx \frac{\pi}{4\theta}$ will result in a state close to $\ket{\psi_1}$.

Amplitude amplification can be used to speed up Grover search by using an informed prior rather than a uniform prior.
Furthermore, it is useful as a subroutine in other algorithms, such as finding the number of \enquote{good} states for a Grover search~\autocite{brassard2002}, quantum Monte Carlo methods~\autocite{montanaro2015} and some bandit algorithms to be discussed in what follows.

\subsection{Amplitude estimation}
\label{sec:amplitude-estimation}
As with amplitude amplification, amplitude estimation considers states that are decomposed into a superposition of two states, and, as the name suggests, estimates the amplitude of one of the states.
Given a state, or more generally the algorithm, $\mathcal{A}$ with which it is generated,
\begin{equation}
    \mathcal{A} \ket{0} = \sqrt{a} \ket{\psi_1} + \sqrt{1-a} \ket{\psi_0},
\end{equation}
where $\ket{\psi_1}$ is a state of interest and $\ket{\psi_0}$ its orthogonal complement, the goal is to estimate its amplitude $a = \lvert\bra{\psi_1}\ket{\psi_1}\rvert^2$.

With its original formulation in~\autocite{brassard2002}, it was proven that the amplitude can be estimated with an additive error of
\begin{equation}
    \epsilon \leq 2\pi \frac{\sqrt{a(1-a)}}{t} +\frac{\pi^2}{t^2}
\end{equation}
with probability at least $8/\pi^2(\approx 81.06\%)$ using $t$ calls to the algorithm $\mathcal{A}$.
The probability can be increased to $1-\delta$, requiring $O(\log(1/\delta))$ calls to $\mathcal{A}$~\autocite{montanaro2015}.
Later variants have been proposed to reduce the qubits needed and circuit depths~\autocite{suzuki2020, nakaji2020, grinko2021}, making it more feasible for NISQ devices, while still achieving similar asymptotic error bounds.


\subsection{Quantum Monte Carlo}
\label{sec:qmc}
Monte Carlo methods have been a powerful tool to analyse the behaviour of quantum mechanical systems, where probabilistic methods are natural to describe probabilistic physics~\autocite{ceperley1986, austin2012,gubernatis2016}.
On the other hand, more in line with the scope of this report, recent advances in quantum computing have opened up a new avenue for the intersection of quantum mechanics and Monte Carlo methods.

In the problem of estimating the mean of a random variable without assumptions of its distribution or properties, the additive error can be bounded by Chebyshev's inequality as
\begin{equation}
    P(\lvert \hat\mu - \mu \rvert \geq \epsilon) \leq \frac{\sigma^2}{n \epsilon^2},
\end{equation}
where $\hat\mu$ is the sample mean, $\mu$ is the true mean, $\sigma$ is the standard deviation and $n$ is the number of samples.
Consequently, there is a need of quadratically many samples to achieve a given error, which for example means that estimating the mean of a random variable with a standard deviation of $1$ with four decimals' accuracy and a certainty of $99.9\%$ would require $10^9$ samples.
Moreover, this is provably optimal asymptotically~\autocite{dagum2000}.

Generalising amplitude estimation, in~\autocite{montanaro2015}, a \enquote{near-quadratic} speed-up of Monte Carlo methods was achieved by using amplitude estimation to estimate the mean of a random variable encoded by quantum algorithms.
Given an algorithm $\mathcal{O}$ whose measurement outputs are assigned real values such that $v(\mathcal{A})$ is a random variable with mean $\mu$ and variance $\sigma^2$, it is proved that approximating $\mu$ with an additive error of $\epsilon$ can be achieved with only $\tilde{O}(\sigma/\epsilon)$ calls to $\mathcal{A}$ and its inverse, which is a near-quadratic speed-up over the classical case\footnotemark{}.

\footnotetext{
    The $\tilde{O}$ notation is the same as regular big-O notation, but with (poly-) logarithmic factors omitted, e.g., $n^2 \log(n)^2 = \tilde{O}(n^2)$.
}

The simpler version on which the general builds, $v(\mathcal{A})$ is assumed to lie in the interval $[0,1]$.
Thus, the value can be encoded in a single qubit by through a unitary
\begin{equation}
    U\ket{x}\ket{0} = \ket{x}\left(\sqrt{1 - \phi(x)}\ket{0} + \sqrt{\phi(x)}\ket{1}\right),
\end{equation}
where $\phi(x)$ is the output of the algorithm were $x$ to be measured.
Thence, the amplitude of the last qubit is simply estimated using amplitude estimation, as described in \cref{sec:amplitude-estimation}, using an appropriate number of iterations and repetitions.
The initial state for the amplitude estimation is set to
\begin{equation}
    \ket{\psi} = U(\mathcal{O} \otimes I)\ket{0}.
\end{equation}

For these bounded random variables in particular, $O(1/\epsilon)$ iterations suffices to achieve an additive error of $\epsilon$, repeating the whole procedure $O(1/\log(\delta))$ times to achieve a certainty $1-\delta$~\autocite{montanaro2015}.