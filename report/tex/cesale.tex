\chapter{Quantum bandits}
\label{chap:qbandits}

Several formulations of the multi-armed bandit problem have been made for a quantum computing setting.
As the central issue in bandit problems lie in sample efficiency rather than computational difficulties, quant-um computers offer little advantage assuming classical bandits.
However, by granting some sort of superposition queries, means can be estimated more efficiently, and so regrets may be reduced, or best arms found more quickly.

Querying in superposition may at first appear to remove any real-world relevance; administering medications to patients can certainly not be done in superposition.
However, with the training for reinforcement learning primarily being done in simulation, it is conceivable that the theory of quantum bandits may be applied to the learning of agents that are trained on quantum hardware and subsequently deployed to the real world.



\section{Best arm identification}
In \cite{casale2020}, an algorithm based on amplitude amplification is proposed and is shown to find the optimal arm with quadratically fewer queries than the best classical algorithm for classical bandits â€” albeit with a significant drawback: the probability of the correct arm being suggested can not be set arbitrarily high, but is instead given by the ratio of the best arm's mean to the sum of the means of all arms.
This greatly limits the usefulness of the algorithm, but it may still serve as a useful baseline for comparison, with the more complicated algorithms discussed in the following sections seeable as extensions hereof.

They assume access to an oracle $O_e$ that encodes the probabilities of the arms,
\begin{equation}
    O_e: \ket{x}\ket{0} \mapsto \ket{x} \left(\sum_y \sqrt{p(y|x)} \ket{y}\right),
\end{equation}
where $x$ is the arm to be queried, $y$ some internal state and $p(y|x)$ the probability of transitioning into state $y$ given the pulling of arm $x$.
For a given arm $x$ and the resulting state $\ket{y}$, the reward is determined by some function $f(x,y) \to \{0, 1\}$, accessed through the phase oracle $O_f$,
\begin{equation}
    O_f :  \ket{x}\ket{y} \mapsto (-1)^{f(x,y)} \ket{x}\ket{y}.
\end{equation}

For such bandits, regret minimisation is no longer a valid objective, as all arms are in a sense pulled simultaneously.
Instead, the problem is to find a strategy that maximises the probability of finding the optimal arm with as few applications of $\mathcal{O_e}$ as possible.

\subsection{Proper best arm identification}
The authors of \cite{wang2021} propose a more sophisticated algorithm, improving the results of \cite{casale2020} by allowing the probability of finding the optimal arm to be set arbitrarily high.

\section{One at a time and regrets}
By instead having one oracle for each arm $i$,
\begin{equation}
    O_i: \ket{0} \mapsto \sum_\omega \sqrt{p_i(\omega)} \ket{\omega} \ket{r_i(\omega)},
    \label{eq:wan_oracle}
\end{equation}
where $\omega$ is the internal state of the arm, sampled according to the measure $p_i$, and $r_i{\omega}$ a random variable that is the reward received when pulling arm $i$ and producing state $\omega$.
The agent then decides for each step in the bandit problem, which oracle to invoke, trying to minimise the cumulative regret, where the means here are defined as
\begin{equation}
    \mu_i = \sum_\omega p_i(\omega) E(y_x(\omega)).
\end{equation}
In \cite{wan2022}, an algorithm for bounded-value arms achieving $O(n \log T)$ regret was proposed, $n$ being the number of arms.

The algorithm proposed is essentially a UCB-like algorithm, where QMC (as described in \cref{sec:qmc}) is used to estimate the means more efficiently.
Because QMC estimates are only produced after a set number of quantum queries, the algorithm must cleverly decide for how many steps to pull each arm in addition to which arm to pull.

As listed in \cref{alg:qucb1}, the algorithm first runs a preliminary phase where the means are estimated using QMC, after which it iteratively pulls the arms with the highest confidence bounds, halving the confidence bound after and doubling the number of QMC samples used to estimate the mean.
A confidence parameter $\delta$ is used to determine the number of QMC samples to use, satisfying $\lvert\hat{\mu}_i - \mu_i\rvert \leq \text{UCB}_i$ with probability $1-\delta$.
The constant $C_1>1$ is only described existentially to give an upper bound to the number of QMC queries needed.

\begin{algorithm}
    \caption{QUCB1 as proposed in \cite{wan2022}}
    \label{alg:qucb1}
    \begin{algorithmic}[1]
        \Require $k$ arms, $\mathcal{O}_i$ as in \cref{eq:wan_oracle}, $T$ horizon, $0 < \delta \ll 1$
        \For {$i = 1, \ldots, k$}
        \State $\text{UCB}_i \gets 1$
        \State $N_i \gets (C_1/\text{UCB}_i) \log(1/\delta)$
        \State Estimate $\mu_i$ using QMC with $N_i$ samples
        \EndFor
        \While{Total number of queries to $\mathcal{O}_i$, $\mathcal{O}_i^\dag$ is less than $T$}
        \State $i \gets \argmax_i (\mu_i + \text{UCB}_i)$
        \State $\text{UCB}_i \gets \text{UCB}_i /2$
        \State $N_i \gets (C_1/\text{UCB}_i) \log(1/\delta)$
        \State Update estimate of $\mu_i$ using QMC with $N_i$ samples
        \EndWhile
    \end{algorithmic}
\end{algorithm}
