
\chapter{Abstract}
The bandit problem is fundamental in sequential decision-making.
In it, an agent must at each step decide between different arms.
These are a fixed set of actions, each with unknown reward distributions.
The goal is to maximise the cumulative rewards received.
Real-world applications include clinical trials, recommender systems, portfolio optimisation and more.
This thesis studies the problem and how it can be solved with modern technologies: quantum computing and neural-network-based reinforcement learning (RL).
These are compared with classical bandit algorithms.

A primary focus is the quantum UCB (upper confidence bounds) algorithm.
It uses a quantum Monte Carlo subroutine to estimate reward means more precisely than is possible classically.
This requires rewards received as quantum-mechanical superpositions.
Comparisons with classical UCB and the Bayesian method of Thompson sampling were conducted.
In addition to new fixed instances, instances drawn from various priors (re Bayesian regret) were used, which is rare in the literature and new for the quantum algorithm.
Results indicate that quantum UCB can outperform classical methods, but only for long time horizons and highly correlated arms.
Further research is needed to determine which applications permit the use of the quantum algorithm and which are difficult enough to warrant its use.

Common RL, not specifically designed for the bandit problem, is tested on it.
It is observed that the algorithms struggle to solve the tested instances.
This is presumably due to poor state representations or rewards.
While future work could find better setups for these algorithms, bandit-specific algorithms will likely remain the best choice for the problem; with the effort that may be required to match the specific algorithms, the benefit of using a general tool is lost.

To conclude, the thesis finds quantum computing useful for the bandit problem, while general RL appears less advantageous.
More studies are needed to determine under which conditions quantum algorithms provide benefits and to test the algorithm on different reward distributions and arm numbers.
Future work can consider different bandit variants or attempt to find a better quantum bandit algorithm by building something inherently quantum rather than extending classical algorithms.



\cleardoublepage