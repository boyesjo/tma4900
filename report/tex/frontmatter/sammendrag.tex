\begin{otherlanguage}{norsk}

  \chapter{Sammendrag}
  Bandittproblemet er et grunnleggende problem i sekvensiell beslutningstaging.
  I det må en spiller velge mellom ulike armer i hver runde.
  Armene er en bestemt mengde valg spilleren kan ta, der hvert valg har en ukjent premiefordeling.
  Målet er derfra å maksimere samtlige mottatte premier.
  Faktiske bruksområder inkluderer kliniske studier, anbefalingssystemer, porteføljeoptimering og mer.
  Her studeres problemet og hvordan det kan løses med moderne teknologier, nemlig kvantedatamaskiner og forsterkende læring (RL) med nevrale nettverk.
  Disse sammenlignes med klassiske, bandittspesifikke algoritmer.

  Kvante-UCB (øvre konfidensintervallgrenser) er en sentral algoritme i oppgaven.
  I den brukes en kvante-Monte-Carlo-prosedyre for å estimere forventet vinning mer nøyaktig enn det man kan klassisk, hvilket krever premier mottatt som kvantemekaniske superposisjonstilstander.
  Sammenligninger med klassisk UCB og den bayesiske metoden Thompson-trekking ble gjort.
  I tillegg til nye fastsatte instanser, er også trukne instanser fra flere priorfordelinger (mht. bayesisk tap) brukt, hvilket er sjeldent i litteraturen og nytt for kvantealgoritmen.
  Resultatene viser at kvante-UCB kan slå klassiske metoder, men kun for vanskelige instanser.
  Kvantefordeler krever mange runder og sterkt korrelerte armer.
  Videre forskning kreves for å finne ut av hvilke anvendelser som tillater bruk av kvantealgoritmen og som er vanskelige nok til å fordre dens bruk.

  Vanlig RL, som ikke er laget spesifikt for bandittproblemet, testes på det.
  Det ses at de sliter med å løse de prøvde instansene.
  Dette skyldes antagelig dårlige tilstandsrepresentasjoner eller premiefunksjoner.
  Selv om fremtidig arbeid kan finne bedre innstillinger for algoritmene, er nok de bandittspesifikke ennå best egnet; ved å bruke krefter på å anpasse algoritmene, mistes fordelen med å bruke generelle verktøy.

  Arbeidet viser at kvantedatamaskiner kan være nyttige for bandittproblemet, mens generell RL tilsynelatende ikke kan det.
  Mer forskning må til for å finne ut av når kvantealgoritmer er fordelaktige og for å teste algoritmen på andre premiefordelinger og antall armer.
  Fremtidige studier kan se på andre bandittvarianter eller forsøke å konstruere en bedre kvantebandittalgoritme ved å lage noe helt kvantemekanisk i stedet for å bygge videre på klassiske algoritmer.

\end{otherlanguage}

\cleardoublepage