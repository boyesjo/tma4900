\section{Oracles as bandit arms}
\label{sec:wan2022}
A way to quantise the bandit problem is to assign to each arm a quantum oracle.
For each arm $a \in \mathcal{A}$, a bandit oracle can be defined as
\begin{equation}
    \mathcal{O}_a: \ket{0}\otimes\ket{0} \mapsto \sum_{\omega \in \Omega} \left(\sqrt{P_a(\omega)} \ket{\omega} \otimes \ket{X_a(\omega)} \right),
    \label{eq:wan_oracle}
\end{equation}
where $\omega$ is some sample space on which $X_a$ is a random variable with probability measure $P_a$.
Applying the oracle to the state $\ket{0}$ produces a superposition of all possible outcomes of the random variable $X_a$, such that measuring the second register will produce a sample from $X_a$.
In this way, this quantum version of the bandit problem can be reduced to the classical case, but by maintaining the superposition, quantum advantages can be gained.

As with classical arms, the agent decides for each step in the bandit problem which oracle to invoke, trying to minimise the cumulative regret, where the means here are defined as
\begin{equation}
    \begin{aligned}
        \mu_a
         & = \sum_{\omega \in \Omega} P_a(\omega) X_a(\omega)                     \\
         & = \bra{00} \mathcal{O}_a^\dagger (I \otimes Z) \mathcal{O}_a) \ket{00} \\
    \end{aligned}
\end{equation}
In~\autocite{wan2022}, an algorithm for bounded-value arms achieving $O(n \log T)$ regret was proposed, $n$ being the number of arms.
For bounded variances, the regret is $O(n \ \text{poly}(\log T))$, which is still substantially better than $\Omega(\sqrt{nT})$ minimax regret for classical bandits.

The algorithm proposed is essentially a UCB-like algorithm, where QMC (as described in \cref{sec:qmc}) is used to estimate means more efficiently.
Because QMC estimates are only produced after a set number of quantum queries, the algorithm must cleverly decide for how many steps to pull each arm in addition to which arm to pull, before running a QMC session.

As listed in \cref{alg:qucb1}, the algorithm first runs a preliminary phase where the means are estimated using QMC with a fixed number of samples, after which it iteratively pulls the arms with the highest confidence bounds, after which the confidence bound are halved and the number of QMC samples to be used for that arm is doubled.
A confidence parameter $\delta$ is used to determine the number of QMC samples to use, satisfying $\lvert\hat{\mu}_i - \mu_i\rvert \leq \text{UCB}_i$ with probability $1-\delta$.
The constant $C_1>1$ is only described existentially to give an upper bound to the number of QMC queries needed, coming from the big-O notation used to describe QMC convergence.
How it should be set for implementation of the algorithm is not described in the paper.

% \begin{algorithm}
%     \caption{QUCB1 as proposed in~\autocite{wan2022}}
%     \label{alg:qucb1}
%     \begin{algorithmic}[1]
%         \Require Set of arms $\mathcal{A}$, $\mathcal{O}_i$ as in \cref{eq:wan_oracle}, $T$ horizon, $0 < \delta \ll 1$
%         \For {$a \in \mathcal{A}$}
%         \State $\text{UCB}_a \gets 1$
%         \State $N_a \gets (C_1/\text{UCB}_a) \log(1/\delta)$
%         \State Estimate $\mu_a$ using QMC with $N_a$ samples
%         \EndFor
%         \While{Total number of queries to the oracles is less than $T$}
%         \State $a \gets \argmax_a (\hat\mu_a + \text{UCB}_a)$
%         \State $\text{UCB}_a \gets \text{UCB}_a /2$
%         \State $N_a \gets (C_1/\text{UCB}_a) \log(1/\delta)$
%         \State Update estimate of $\mu_a$ using QMC with $N_a$ samples
%         \EndWhile
%     \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
    \SetAlgoLined
    \caption{QUCB1}
    \label{alg:qucb1}
    \KwIn{Set of arms $\mathcal{A}$, $\mathcal{O}_i$ as in \cref{eq:wan_oracle}, $T$ horizon, $0 < \delta \ll 1$}
    \For {$a \in \mathcal{A}$}{
        $\text{UCB}_a \gets 1$\;
        $N_a \gets (C_1/\text{UCB}_a) \log(1/\delta)$\;
        Estimate $\mu_a$ using QMC with $N_a$ samples\;
    }
    \While{Total number of queries to the oracles is less than $T$}{
        $a \gets \argmax_a (\hat\mu_a + \text{UCB}_a)$\;
        $\text{UCB}_a \gets \text{UCB}_a /2$\;
        $N_a \gets (C_1/\text{UCB}_a) \log(1/\delta)$\;
        Update estimate of $\mu_a$ using QMC with $N_a$ samples\;
    }
\end{algorithm}

\subsection{Proof of logarithmic regret}
Recall that for variables in $Y \in [0, 1]$ producible by quantum algorithms, QMC can estimate $\mathbb{E}[Y]$ with error $\lvert\hat{\mu} - \mu\rvert \leq \epsilon$ with probability $1-\delta$ using $\frac{C_1}{\epsilon} \log \frac{1}{\delta}$ queries or less to the oracle or its adjoint, for some universal constant $C_1>1$.
That means that for each $a$,
\begin{equation}
    \lvert\hat{\mu}_a - \mu_a\rvert \leq \text{UCB}_a
    \quad \text{with probability} \quad
    1 - \delta,
    \label{eq:wan_qmc}
\end{equation}
for each QMC session in the algorithm.

Let $S_a$ be the set of stages where arm $a$ is pulled in the while-block of the algorithm, and let its cardinality be $K_a = |S_a|$.
Then, each arm is pulled $(2^{K_a + 1} - 1)C_1 \log \frac1\delta$ in the second phase.
With the preliminary phase, the total number of queries is
\begin{equation}
    C_1 \log \frac{1}{\delta} \sum_{a \in \mathcal{A}} 2^{K_a + 1} = T.
\end{equation}

Further, using Jensen's inequality,
\begin{equation}
    \sum_{a \in \mathcal{A}} 2^{K_a + 1} \geq k 2^{1/k \sum_{a \in \mathcal{A}} (K_a + 1)},
\end{equation}
it follows that
\begin{equation}
    \sum_{a \in \mathcal{A}} K_a
    \leq
    k \log_2 \left(\frac{T}{k C_1 \log \frac{1}{\delta}}\right) -k,
\end{equation}
which considering the first $k$ rounds, gives that the total number of QMC sessions is
\begin{equation}
    N_{\text{QMC}} \leq k \log_2 \left(\frac{T}{k C_1 \log \frac{1}{\delta}}\right).
\end{equation}

The probability of all QMC queries satisfying the error bound of \cref{eq:wan_qmc} is
\begin{equation}
    \begin{aligned}
        1 - \left(\bigcup_{i = 1}^{N_{\text{QMC}}} P(\text{Query } i \text{ fail}) \right)
         & \leq
        1 - \sum_{i = 1}^{N_{\text{QMC}}} P(\text{Query } i \text{ fail})             \\
         & =
        1 - \sum_{i = 1}^{N_{\text{QMC}}} \delta                                      \\
         & =
        1 - N_{\text{QMC}} \ P(\text{Query } i \text{ fail})                          \\
         & \leq k \log_2 \left(\frac{T}{k C_1 \log \frac{1}{\delta}}\right) \ \delta.
    \end{aligned}
\end{equation}
Denote this event by $E$.
For the $a$ that is chosen in the argmax in line 6 of the algorithm,
\begin{equation}
    \hat\mu_a + \text{UCB}_a \geq \hat\mu^* + \text{UCB}^*,
\end{equation}
and given $E$, it generally holds that
\begin{equation}
    \hat\mu \leq \mu + \text{UCB}
    \quad \text{and} \quad
    \mu \leq \hat\mu + \text{UCB}.
\end{equation}
Hence, the suboptimality gap of arm $a$ is bounded by
\begin{equation}
    \begin{aligned}
        \Delta_a & := \mu^* - \mu_a                                             \\
                 & \leq (\hat\mu^* + \text{UCB}^*) - (\hat\mu_a - \text{UCB}_a) \\
                 & \leq (\hat\mu_a + \text{UCB}_a) - (\hat\mu_a - \text{UCB}_a) \\
                 & = 2 \text{UCB}_a.
    \end{aligned}
\end{equation}
Notably, this holds regardless of what stage of the algorithm the arm is pulled in, so the last, most precise estimate of $\mu_a$ is used, wherein $\text{UCB}_a = 2^{1 - K_a}$.
Consequently, the regret contribution of arm $a$ is bounded by
\begin{equation}
    \begin{aligned}
        T_a \Delta_a & \leq 2 T_a \text{UCB}_a.                            \\
                     & = 2 (2^{K_a + 1} C_1 \log \frac1\delta) 2^{1 - K_a} \\
                     & = 2^3 C_1 \log \frac1\delta.
    \end{aligned}
\end{equation}
Given $E$, it is hence clear that
\begin{equation}
    \begin{aligned}
        \sum_{a \in \mathcal{A}} T_a \Delta_a  \leq 8 (k-1) C_1 \log \frac1\delta.
    \end{aligned}
\end{equation}

In the case where $E$ does not occur, the regret contribution can be bounded by
\begin{equation}
    \begin{aligned}
        \sum_{a \in \mathcal{A}} T_a \Delta_a \leq T \max_{a\in \mathcal{A}} \Delta_a \leq T.
    \end{aligned}
\end{equation}

The regret can hence be bounded by
\begin{equation}
    \begin{aligned}
        R_T
         & =
        \mathbb{E}\left[ \sum_{a \in \mathcal{A}} T_a \Delta_a \right]
        \\
         & =
        \mathbb{E}\left[ \mathbb{E}\left[ \sum_{a \in \mathcal{A}} T_a \Delta_a \ \middle| \ E \right] \right]
        \\
         & = P(E) \mathbb{E}\left[ \sum_{a \in \mathcal{A}} T_a \Delta_a \ \middle| \ E \right] + P(\neg E) \mathbb{E}\left[ \sum_{a \in \mathcal{A}} T_a \Delta_a \ \middle| \ \neg E \right]
        \\
         & \leq
        (1 - P(\neg E)) \left( 8(k-1)C_1 \log \frac1\delta \right)
        + P(\neg E) T
        \\
         & \leq
        8(k-1)C_1 \log \frac1\delta + P(\neg E) T.
        \\
         & \leq
        8(k-1)C_1 \log \frac1\delta + k\delta \log_2 \left(\frac{T}{kC_1 \log \frac1\delta}\right) T.
    \end{aligned}
\end{equation}
Setting $\delta = 1/T$,
\begin{equation}
    \begin{aligned}
        R_T
         & \leq
        8(k-1)C_1 \log \frac1\delta + k \log_2 \left(\frac{T}{kC_1 \log \frac1\delta}\right)
        \\
         & \leq
        8(k-1)C_1 \log T + k \log_2 \left(\frac{T}{kC_1 \log T}\right)
        \\
         & \leq
        8(k-1)C_1 \log_2 T + k \log_2 T
        \\
         & =
        (8(k-1)C_1 + k) \log_2 T
        \\
         & =
        O(k \log T)
    \end{aligned}
\end{equation}
as desired.