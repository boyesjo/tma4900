\section{Optimality}
In the realm of multi-armed banditry, expressing optimality is fraught with difficulties.
Any precise formulation is contingent upon not only the assumptions made, but also the particular instance, namely actual means and any other parameters.

\subsection{Instance-dependent lower bound}
In order to meaningfully define a lower bound, it is imperative to assume a reasonable algorithm.
Otherwise, trivial policies, such as always pulling the first arm, could achieve zero regret, hindering any meaningful comparison.
One may therefore impose asymptotic consistency, which by definition means that for all inferior arms and all $\alpha \in (0, 1)$,
\begin{equation}
    E(R(T)) = o(T^{\alpha}),
\end{equation}
where $N_i(T)$ is the number of times arm $i$ have been pulled in the first $T$ turns.
For algorithms obeying this property, the Lai-Robbins bound holds \cite{lai1985}, namely that
\begin{equation}
    \lim_{T\to\infty} \inf \frac{E(N_i(T))}{\ln T} \geq \frac{1}{D(P_i || P^*)},
\end{equation}
where $P_i$ is the reward distribution of arm $i$, $P^*$ that of the optimal distribution and $D(\cdot || \cdot)$ is the Kullback-Leibler divergence.
Effectively then, any asymptotically consistent algorithm will suffer a regret of at least $\Omega(\ln T)$.
The Lai-Robbins bound is instance-dependent through its dependence on the Kullback-Leibler divergences.
Its dependence on the divergences which are not known makes it inapplicable to real-world problems, and as reward distributions approach the optimal distribution, the bound diverges.

\subsection{Instance-independent lower bound}
A more general lower bound is the minimax regret.
Given some problem class $\mathcal{E}$, it is defined as
\begin{equation}
    \inf_{\pi \in \Pi} \sup_{\nu \in \mathcal{E}} E(R(\nu, \pi)),
\end{equation}
where $\pi$ is the algorithm and $\nu$ is the problem instance.
The minimax regret is a lower bound on the whole class rather than one particular instance; algorithms may achieve better in some or even most instances, but no algorithm can do better than the minimax regret in all.
In \cite{lattimore2020}, it is proven that for all algorithms, given a fixed horizon $T$ and number of arms $K$, there is at least one problem instance such that
\begin{equation}
    E(R(T)) \geq \Omega(\sqrt{KT}).
\end{equation}
Such a bound is independent of the reward distributions, and as such, it is applicable to real-world problems, but may be overly robust.
It may be preferable to sacrifice performance on some instances to gain performance on others.