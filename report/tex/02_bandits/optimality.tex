\section{Optimality}
\label{sec:optimality}
In the realm of multi-armed banditry, expressing optimality is fraught with difficulties.
Any precise formulation is contingent upon not only the assumptions made, but also the particular instance, namely actual means and distributions overall.
Usually, in the literature, no hyper-distributions are placed on the bandit classes $\mathcal{E}$, so no average regret over all instances can be defined.
Lower bounds resort then to either determine what a reasonable policy can achieve on a given instance, or to describing its worst performance over all instances in the class.
However, by defining some weighted average over all instances, an exact performance measure can be defined, such that optimality is well-defined.
This is rarely done in the literature, but is included here to provide a more complete and practical measure of performance.

\subsection{Instance-dependent lower bound}
In order to meaningfully define a lower bound for a given instance, it is imperative to assume a reasonable algorithm.
Otherwise, trivial policies, such as always pulling the first arm, could achieve zero regret, hindering any meaningful comparison.
The common assumption is that the algorithm is asymptotically consistent in some class $\mathcal{E}$, which by definition means that for all inferior arms and all $\eta \in (0, 1)$, it holds that\footnotemark
\begin{equation}
    \mathbb{E}[T_a] = o(T^{\eta}),
\end{equation}
for all instances $\nu \in \mathcal{E}$.

\footnotetext{
    The small-o is used to denote asymptotic upper bounds and is similar to big-O, but with a strict inequality.
    E.g., $x^2 = o(x^3)$, but $x^2 \neq o(x^2)$.
}

For asymptotically consistent and bandit classes with reward distributions parametrised by only one parameter, the Lai-Robbins bound~\autocite{lai1985} holds.
It states that
\begin{equation}
    \liminf_{T\to\infty} \frac{\mathbb{E}[T_a]}{\ln T} \geq \frac{1}{D(P_a \mathrel{\Vert} P^*)},
    \label{eq:lai-robbins-times}
\end{equation}
where $P_a$ is the reward distribution of arm $a$, $P^*$ that of the optimal distribution and $D(\cdot \mathrel{\Vert} \cdot)$ the Kullback-Leibler divergence.
The Kullback-Leibler divergence is a measure of the difference between two probability distributions over the same space $\mathcal{X}$, defined as
\begin{equation}
    D(P \mathrel{\Vert} Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
\end{equation}
for discrete distributions and
\begin{equation}
    D(P \mathrel{\Vert} Q) = \int_{\mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} \, dx
\end{equation}
in the continuous case.
For more general bandit classes, given finite means, the Kullback-Leibler in the denominator of \cref{eq:lai-robbins} is instead taken to the distribution in that class which is closest to $P_a$ and with mean equal to the mean of $P^*$,
\begin{equation}
    \liminf_{T\to\infty} \frac{\mathbb{E}[T_a]}{\ln T} \geq \frac{1}{d(P_a, \mu^*, \mathcal{E})},
    \label{eq:lai-robbins}
\end{equation}
where
\begin{equation}
    d(P_a, \mu^*, \mathcal{E}) =
    \inf_{P \in \nu \in \mathcal{E} \text{ for some } \nu}
    \left\{ D(P_a \mathrel{\Vert} P): \mathbb{E}[P_a] > \mu^* \right\}.
\end{equation}


From \cref{eq:regret2}, it follows that
\begin{equation}
    \liminf_{T\to\infty} \frac{R_T}{\ln T}
    \geq
    \sum_{a \in \mathcal{A}}\frac{\Delta_a}{d(P_a, \mu^*, \mathcal{E})}.
    \label{eq:lai-robbins-regret}
\end{equation}
Algorithms satisfying \cref{eq:lai-robbins-regret} with equality are said to be asymptotically optimal.

The Lai-Robbins bound is instance-dependent through its dependence on the Kullback-Leibler divergences.
Its dependence on the divergences which are not known makes it inapplicable to real-world problems,
and as reward distributions approach the optimal distribution, the bound diverges.
It is still a useful tool for theoretical analysis and simulated experiments.


\subsection{Instance-independent lower bound}
A more general lower bound is the minimax regret.
Given some problem class $\mathcal{E}$, it is defined as
\begin{equation}
    \Inf_{\pi} \sup_{\nu \in \mathcal{E}} R(\nu, \pi),
\end{equation}
where $\pi$ is the policy, and $\nu$ is the problem instance.
The minimax regret is a lower bound on the whole class rather than one particular instance; algorithms may achieve better in some or even most instances, but no algorithm can do better than the minimax regret for all.
In~\autocite{auer2002a}, it is proven that for all algorithms, given a fixed horizon $T$ and number of arms $K$, there is at least one problem instance such that\footnotemark
\begin{equation}
    R_T = \Omega(\sqrt{KT}).
\end{equation}
Such a bound is independent of the reward distributions, and as such, it is applicable in practice, but it may be overly robust.
It can be preferable to sacrifice performance on some instances to gain performance on others.
Minimax regret optimality implies a flat risk profile, while in practice, performance may be desired to correlate with instance difficulty.
Surprisingly, minimax optimality does not negate instance optimality, and recent algorithms have been shown to achieve both~\autocite{menard2017, jin2020}.

\footnotetext{
    The $\Omega$-notation is used to denote asymptotic lower bounds, effectively the opposite of big-O.
    E.g., $x^2 + x = \Omega(x^2) = \Omega(x) \neq \Omega(x^3)$.
    \label{fn:omega}
}

\subsection{Bayesian optimality}
\label{sec:bayesian-optimality}
If a prior distribution were to be placed on the reward distributions, a notion of average or Bayesian regret can be defined.
Alternatively, this can be thought of as having some weighing of the particular instances and averaging over them.
Following the notation of~\autocite{lattimore2020}, the Bayesian interpretation is what is here presented.

For some bandit class $\mathcal{E}$, one simply includes the prior distribution of the reward distributions in the expectation taken to define the Bayesian regret,
\begin{equation}
    \text{BR}_T(Q, \pi)
    = \mathbb{E}_{\pi, \nu, Q} \left[ \sum_{a \in \mathcal{A}} \Delta_a T_a \right]
    = \mathbb{E}_{\nu \sim Q} \left[R_T(\nu, \pi) \right],
\end{equation}
where $Q$ is the prior distribution over $\mathcal{E}$.
Is it trivially observed that the Bayesian regret bounded above by the minimax regret.
However, it can be proven that there exists priors such that the Bayesian regret is bounded below by some $\Omega(\sqrt{KT})$~\autocite{lattimore2020}, such that\footnotemark
\begin{equation}
    \sup_{Q} \inf_{\pi} \text{BR}_T(Q, \pi) = \Theta(\sqrt{KT}).
\end{equation}

\footnotetext{
    The $\Theta$-notation describes both upper and lower bounds, effectively the intersection of big-O and $\Omega$ (q.v. \cref{fn:omega}).
    E.g., $x^2 + x$ is $\Theta(x^2)$, but not $\Theta(x)$ or $\Theta(x^3)$.
}


The Bayesian regret can be a useful tool for designing algorithms, as knowledge inlaid in the prior can be used to guide the algorithm, but it may lead to less robust designs than policies devised by the above discussed methods.
Furthermore, calculating the optimal policy is generally intractable for any horizon of size and reward distributions more complicated than Bernoulli~\autocite{lattimore2020}.
In simpler cases, dynamic programming can be fruitful.
With Bernoulli bandits and clever implementations, optimal strategies can indeed be found~\autocite{pilarski2021}.
For longer horizons, discounting, namely reducing the weight of future rewards, can be used to make the problem tractable or at least easier to approximate well.
Still, direct optimisation is restrained to the regimes of small horizons and simple reward distributions, and for more complicated problems, heuristic methods are required.
