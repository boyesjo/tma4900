\section{Classical solutions}
\begin{table}[tbp]
    \centering
    \captionabove[
        Strategies for the multi-armed bandit problem.
    ]
    {
        Strategies for the unstructured, stochastic multi-armed bandit problem.
        The minimax regret is the instance-independent upper bound, while the regret is the dependent.
        Note that different UCB variants with different regret bounds exist, with UCB1 only achieving the listed bounds for rewards in $[0, 1]$.
        Also note that the Thompson sampling properties listed assumes Bernoulli rewards and a uniform prior.
    }
    \label{tab:strategies}
    \begin{tblr}{
            width=\linewidth,
            colspec={X[l] X[r] X[r] X[r]}
        }
        \toprule
        Strategy       & Minimax regret       & Regret      & Tuning    \\
        \midrule
        Random         & $O(T)$               & $O(T)$      & NA        \\
        Greedy         & $O(T)$               & $O(T)$      & NA        \\
        Epsilon-greedy & $O(T)$               & $O(T)$      & Difficult \\
        Epsilon-decay  & $O(T)$               & $O(\log T)$ & Difficult \\
        UCB1           & $O(\sqrt{T \log T})$ & $O(\log T)$ & Barely    \\
        Thompson       & $O(\sqrt{T})$        & $O(\log T)$ & Priors    \\
        \bottomrule
    \end{tblr}
\end{table}

\subsection{Explore-only}
Pure exploration is obviously a suboptimal strategy, but it is a good baseline against which to compare.
It can be implemented by selecting an arm uniformly or in order, performing poorly either way.
A random arm-selection procedure is described by \cref{alg:random}.

\begin{algorithm}
    \caption{Random arm selection}
    \label{alg:random}
    Sample $a$ from $\mathcal{A}$ uniformly\;
    \Return $a$\;
\end{algorithm}

It is easy to understand that the regret is
\begin{equation}
    R_T = T\left(\mu^* - \frac{1}{k}\sum_{i=1}^k \mu_i\right),
\end{equation}
which is necessarily linear in $T$.
This motivates the search for an algorithm with sublinear regret.
It should be clear that a linear regret is actually the worst possible.

\subsection{Greedy}
Tending away from pure exploration to exploitation, a greedy algorithm will always select the arm with the highest empirical mean.
Here, all arms are pulled an initial $m \geq 1$ times, after which estimated means are used to select the best arm.
Then, the arm with the highest empirical mean is selected for all remaining turns.
The arm-selection procedure is listed in \cref{alg:greedy}, where $\hat{\mu}_a$ is the estimated mean of arm $a$.

\begin{algorithm}
    \caption{Greedy arm selection}
    \label{alg:greedy}
    \If{$t \leq mk$}{
        \Return $(t \mod k) + 1$\;
    } \Else {
        \Return $\argmax_{a \in \mathcal{A}} \hat{\mu}_a$\;
    }
\end{algorithm}


With greedy selection, the expected regret is clearly still linear in the horizon, as there is a non-zero probability of selecting the wrong arm when taking the greedy action.
Still, there is a chance of achieving zero regret and the constant factor is reduced compared to pure exploration selection.
To improve hereupon, it is necessary to occasionally explore other arms, which leads into the epsilon-greedy algorithm.

\subsection{Epsilon-greedy}
The problem with the greedy algorithm is that it may be unlucky and not discover the best arm in the initial exploration phase.
To mitigate this, the epsilon-greedy algorithm can be used.
In this algorithm, the arm that is presumed to be best is pulled with probability $1-\epsilon$, while in the other $\epsilon$ proportion of the turns, the arm is selected uniformly at random.
This ensures convergence to correct exploitation as the horizon increases, and it will generally reduce the regret.

Still, with a constant $\epsilon$, a constant proportion of the turns will be spent exploring, keeping the regret necessarily linear in the horizon.
Choosing $\epsilon$ is a trade-off between exploration and exploitation and can significantly affect the regret.

\begin{algorithm}
    \caption{Epsilon-greedy arm selection}
    \label{alg:eps_greedy}
    \eIf{$t \leq mk$}{
    \Return $(t \mod k) + 1$\;
    }{
    Sample $u$ from $[0,1)$ uniformly\;
    \eIf{$u < \epsilon$}{
        Sample $a$ from $\mathcal{A}$ uniformly\;
        \Return $a$\;
    }{
        \Return $\argmax_{a \in \mathcal{A}}\hat{\mu}_a$\;
    }
    }
\end{algorithm}

\subsubsection{Epsilon-decay}
To remedy the linear term in the regret, modifications to the epsilon-greedy algorithm have been proposed wherein $\epsilon$ is a function of the current time step $t$.
Specifically, in order to achieve sublinear regret, it is necessary to decay $\epsilon$ towards zero.
Decreasing $\epsilon$ over time should make intuitive sense; exploration is more crucial in the early stages of the algorithm, whereas exploitation is more important when the agent has more reliable estimates of the reward means.
For example, one successful strategy is to set $\epsilon \propto 1/t$, which has been shown to achieve logarithmic instance-dependent regret~\autocite{auer2002}.
It is worth noting, however, that the optimal decay rate depends on the specific instance, and achieving logarithmic regret can be challenging in practice~\autocite{bubeck2012}.


\subsection{Upper confidence bounds}
\label{sec:ucb}
The upper confidence bound (UCB) algorithm is a family of more sophisticated algorithm based on estimating upper confidence bounds for the means of all arms, and they are a common baseline for more advanced algorithms tackling more complicated bandit variants~\autocite{li2010}.
The arm whose upper confidence bound is highest is always pulled, a principle known as \enquote{optimism in the face of uncertainty}.
This should make sense, as if the wrong arm appears best, it will be pulled more often and the empirical mean will be corrected, while the true best arm with its larger bound will eventually become highest and so pulled.
When exploiting the actual best arm, the agent can trust it to be the best, as the confidence bound will remain above those of all the other arms.
In addition, by increasing the confidence as the number of pulls increases, getting stuck in suboptimality is avoided.

\subsubsection{UCB1}
Assuming rewards in $[0,1]$ and using Hoeffding's inequality, it follows that
\begin{equation}
    p
    = P(\mu_a > \hat{\mu_a} + \text{UCB}_a)
    \leq \exp(-2T_a \text{UCB}_a^2),
\end{equation}
where $\text{UCB}_a$ is the upper confidence bound for arm $a$ and $T_a$ is the number of times arm $a$ has been pulled.
Solving for $\text{UCB}_a$ yields
\begin{equation}
    \text{UCB}_a = \sqrt{\frac{-\ln p}{2T_a}}.
\end{equation}
Letting $p(t) = t^{-4}$ gives
\begin{equation}
    \text{UCB}_a = \sqrt{\frac{2 \ln t}{T_a}},
\end{equation}
which is a common choice for the upper confidence bound, leading to the UCB1-algorithm.
In~\autocite{auer2002}, it is shown that UCB1 achieves
\begin{equation}
    R_T
    \leq
    \left( 8 \sum_{a \in \mathcal{A} \setminus a^*} \frac{\log T}{\Delta_a} \right)
    + \left(1 + \frac{\pi^2}{3}\right) \sum_{a \in \mathcal{A}} \Delta_a,
\end{equation}
which misses instance-dependent optimality only by some constant factor.
Nor is UCB1 truly minimax-optimal, as its instance-independent upper bound is $O(\sqrt{kT \log T})$~\autocite{bubeck2012}.

\begin{algorithm}
    \caption{UCB arm selection}
    \label{alg:ucb}
    \If{$t \leq k$}{
        \Return $t$\;
    }{
    } \Else {
        \Return $\argmax_{a \in \mathcal{A}} (\hat{\mu}_a + \text{UCB}_a)$\;
    }
\end{algorithm}


Regardless of the assumptions made and the bandit class, the procedure for UCB strategies follows as in \cref{alg:ucb}.
For few arms, the process is easily visualised, as is done in \cref{fig:ucb}.
There it is made clear how the confidence bound based strategy naturally exploits the best arm and how the poorer arms are not explored more than needed.

Many variants of the algorithm exist; different assumptions about the distributions change the confidence bounds.
While the choice of $p$ is arbitrary, it is less of nuisance than the choice of $\epsilon$ in the epsilon-greedy algorithm, with specific choices of $p$, such as the UCB1-algorithm, being well-studied and known to perform well.
For example, MOSS, a modification of UCB1, has be shown to minimax-optimal for $\mathcal{E}_{[0,1]}^k$~\autocite{audibert2009}.
Further, incorporating estimates of second moments improve performance in some cases~\autocite{audibert2009a}, while incorporating the whole empirical distributions of observed rewards appears be the most effective approach~\autocite{maillard2011}.


\begin{figure}[p]
    \centering
    \vspace{1 cm}
    \foreach \turn in {10, 100, 1000, 10000} {
            \begin{subfigure}{0.4\textwidth}
                \centering
                \begin{tikzpicture}
                    \begin{axis}[
                            width=\textwidth,
                            height=\textwidth,
                            xlabel=Arm,
                            ylabel=Estimate,
                            grid=major,
                            xtick=data,
                            title={Turn $\turn$},
                            title style={font=\footnotesize},
                        ]
                        \pgfplotstableread[col sep=comma]{../code/bandit_figs/ucb/ucb_\turn.dat}\datatable
                        \addplot[
                            red,
                            only marks,
                            error bars/.cd,
                            y dir=plus,
                            y explicit,
                        ] table[
                                x=arm,
                                y=estimate,
                                y error=ucb,
                                col sep=comma,
                            ] {\datatable};
                        \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.8, 0.2) (1.2, 0.2)};
                        \addplot+[dashed, mark=none, black, forget plot] coordinates {(1.8, 0.4) (2.2, 0.4)};
                        \addplot+[dashed, mark=none, black, forget plot] coordinates {(2.8, 0.7) (3.2, 0.7)};
                    \end{axis}
                \end{tikzpicture}
                % \caption{
                %     Turn $\turn$.
                % }
            \end{subfigure}
            \vspace{0.5cm}
        }
    \caption[
        UCB1 algorithm visualisation.
    ]
    {
        Visualisation of the UCB1 algorithm on three Bernoulli arms; estimates and error bars for each arm at different turns.
        The dots represent the estimates of the means of the arms, while the error bars represent the upper confidence bounds and the dashed lines represent the true means.
        As the number of turns increases, the estimate of the highest mean converge quickly to the true mean, while the estimates of suboptimal arms remain uncertain.
    }
    \label{fig:ucb}
    \vspace{2 cm}
\end{figure}




\subsection{Thompson sampling}
\label{sec:thompson}
Thompson sampling is a Bayesian approach to the multi-armed bandit problem, being the original approach to the problem~\autocite{thompson1933} in 1933, though only in the case of two arms and Bernoulli rewards and without any theoretical guarantees.
This method is noteworthy for its ability to incorporate Bayesian modelling concepts into the fundamentally frequentist problem of multi-armed banditry.

The idea is to sample from the posterior distribution of the means of the arms and pull the arm with the highest sample, as described \cref{alg:thompson}.
With Bernoulli rewards and uniform priors, which is a special case of the Beta distribution, the posterior distribution is conjugate, namely also a Beta distribution, from which samples are easily drawn.
The posterior distribution is updated after each pull, using the observed reward as evidence.
By graphing the posterior distribution, such as is done in \cref{fig:thompson}, it is possible to see how the algorithm efficiently exploits the best arm while giving enough explorative efforts.

\begin{figure}[p]
    \pgfplotsset{
        ylabel right/.style={
                after end axis/.append code={
                        \node[rotate=90, anchor=north]
                        at (rel axis cs:1.1,0.5)
                        {\footnotesize #1};
                    }
            }
    }

    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 4,
                    },
                width=0.35\textwidth,
                height=0.3\textwidth,
                no markers,
                grid=major,
                title style={font=\footnotesize},
            ]
            \nextgroupplot[
                ylabel=Probability,
                title=Arm 1,
            ]
            \addplot[red] table[
                    x=x,
                    y=y1,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_0.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.2, 0) (0.2, 2)};

            \nextgroupplot[
                title=Arm 2,
            ]
            \addplot[red] table[
                    x=x,
                    y=y2,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_0.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.4, 0) (0.4, 2)};

            \nextgroupplot[
                title=Arm 3,
                ylabel right=Priors,
            ]
            \addplot[red] table[
                    x=x,
                    y=y3,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_0.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.7, 0) (0.7, 2)};


            % row 2
            \nextgroupplot[
                ylabel = Probability,
            ]
            \addplot[red] table[
                    x=x,
                    y=y1,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_10.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.2, 0) (0.2, 2.5)};

            \nextgroupplot
            \addplot[red] table[
                    x=x,
                    y=y2,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_10.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.4, 0) (0.4, 2.5)};

            \nextgroupplot[
                ylabel right = Turn 10,
            ]
            \addplot[red] table[
                    x=x,
                    y=y3,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_10.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.7, 0) (0.7, 4)};

            % row 3
            \nextgroupplot[
                ylabel = Probability,
            ]
            \addplot[red] table[
                    x=x,
                    y=y1,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_100.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.2, 0) (0.2, 3)};

            \nextgroupplot[
            ]
            \addplot[red] table[
                    x=x,
                    y=y2,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_100.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.4, 0) (0.4, 4)};

            \nextgroupplot[
                ylabel right = Turn 100,
            ]
            \addplot[red] table[
                    x=x,
                    y=y3,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_100.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.7, 0) (0.7, 11)};

            % row 4
            \nextgroupplot[
                xlabel = Reward mean,
                ylabel = Probability,
            ]
            \addplot[red] table[
                    x=x,
                    y=y1,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_1000.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.2, 0) (0.2, 4)};

            \nextgroupplot[
                xlabel = Reward mean,
            ]
            \addplot[red] table[
                    x=x,
                    y=y2,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_1000.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.4, 0) (0.4, 4)};

            \nextgroupplot[
                xlabel = Reward mean,
                ylabel right = Turn 1000,
            ]
            \addplot[red] table[
                    x=x,
                    y=y3,
                    col sep=comma,
                ] {../code/bandit_figs/thompson/thompson_1000.dat};
            \addplot+[dashed, mark=none, black, forget plot] coordinates {(0.7, 0) (0.7, 30)};


        \end{groupplot}
    \end{tikzpicture}
    \caption[
        Thompson sampling visualisation.
    ]
    {
        Thompson sampling algorithm with Bernoulli rewards and three arms visualised.
        In the top row, the uniform priors for the reward distribution means are on display.
        Thereunder, the posteriors are shown at turns 10, 100 and 1000.
        The dashed vertical lines indicate the true means.
        As the algorithm progresses, the posterior of the optimal arm quickly spikes and approaches the true mean, while the other remain rather wide, but importantly with almost all their mass less than the optimal arm posterior.
    }
    \label{fig:thompson}
\end{figure}


It was first in 2012, 79 years after its introduction, that Thompson sampling was proven asymptotically optimal for Bernoulli rewards with uniform priors~\autocite{kaufmann2012}
Namely, for every $\epsilon > 0$, there exists a constant $C_\epsilon$ such that
\begin{equation}
    R_T
    \leq
    (1 + \epsilon)
    \sum_{a \in \mathcal{A} \setminus a^*}
    \Delta_a \frac{\log T - \log \log T}{D(P_a \mathrel{\Vert} P^*)}
    + C_\epsilon,
\end{equation}
where $P_a$ is the reward distribution of arm $a$ and $P^*$ is the reward distribution of the best arm.
Meanwhile, the minimax regret is can be bounded by either $O(\sqrt{kT \log T})$~\autocite{agrawal2013} or $O(\sqrt{kT \log k})$~\autocite{agrawal2017}, neither of which is quite minimax-optimal.

Also for Gaussian rewards, it was proven asymptotically optimal with uniform priors~\autocite{honda2014}.
Notably, the Jeffreys prior was shown to be inadequate in achieving optimal regret, highlighting the importance of the prior selection for the algorithm's performance.

\begin{algorithm}
    \caption{Thompson sampling arm selection}
    \label{alg:thompson}
    Update posterior for arm $a_{t-1}$ with reward $X_{t-1}$\;
    \For{$a \in \mathcal{A}$}{
        Sample $\theta_a \sim P(\mu_a \mid \mathcal{D})$\;
    }
    \Return $\argmax_{a \in \mathcal{A}} \theta_a$\;
\end{algorithm}

One of the key advantages of Thompson sampling is that it can natively incorporate prior knowledge about the arms, whereas doing so with the above methods would require some sort of ad-hoc manipulation of the recorded rewards and arm pull counts.
Furthermore, empirical results generally indicate better performance than UCB variants~\autocite{kaufmann2012}.
Still, Thompson sampling is not without its drawbacks.
The algorithm can be computationally expensive, as it requires sampling from the posterior distribution for each arm at each time step.
Even with conjugate priors, the computational costs of sampling will be higher than the straightforward computations required by UCB and epsilon-greedy algorithms.
It is also at its core a probabilistic policy, which for some applications may be undesirable, for example when explainability is a major concern.

\subsection{The doubling trick}
Given an algorithm reliant on knowing the horizon $T$, it is possible to use the doubling trick to achieve similar regret properties regardless of the horizon, and so turn the algorithm into an anytime algorithm.
The doubling trick is a simple idea: simply run the algorithm for $T$ steps first, then $2T$ steps, $4T$, ad infinitum, possibly with some other geometric factor.
It was first introduced in~\autocite{auer1995}, and has since been proven to conserve minimax regrets, but not instance-dependent regrets~\autocite{besson2018}.
Using instead exponential progression of the episode lengths, it is possible to maintain instance-dependent regret bounds instead of minimax optimality~\autocite{besson2018}.

\subsection{Comparison}
\label{sec:comparison}
As a quick comparison of the algorithm presented above, the regrets for a simple bandit instance is shown in \cref{fig:comparison}.
Clearly, random selection performs the worst, as expected, while the greedy algorithm performs somewhat better, but still worse than the $\epsilon$-greedy algorithm.
All of these also appears to have linear regret curves, which is expected.
The UCB algorithm performs better still, while Thompson sampling is without question dominant.
Indeed, for at least the instance shown, the performance hierarchy indicated in this chapter is demonstrated.

\begin{figure}[p]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                width=10cm,
                height=8cm,
                legend entries={
                        Random,
                        Greedy,
                        $\epsilon$-greedy,
                        UCB,
                        Thompson
                    },
                % legend style={at={(-0.2,0.5)},anchor=east},
                legend pos=north west,
                ylabel={Regret},
                xlabel={Turn},
                mystyle,
            ]
            \pgfplotstableread[col sep=comma]{../code/MAB/comparison.dat}\datatable

            \addplot[color=black, mark=none, dashed] table [x=turn, y=Random]{\datatable};
            \addplot[color=blue, mark=none] table [x=turn, y=Greedy]{\datatable};
            \addplot[color=orange, mark=none] table [x=turn, y=EpsilonGreedy]{\datatable};
            \addplot[color=red, mark=none] table [x=turn, y=UCB]{\datatable};
            \addplot[color=green, mark=none] table [x=turn, y=ThompsonBernoulli]{\datatable};

        \end{axis}
    \end{tikzpicture}
    \caption[
        Comparison of bandit algorithms.
    ]
    {
        Comparison of bandit algorithms on a simple bandit instance of two Bernoulli arms with means $0.2$ and $0.8$.
        The algorithms were run for 1000 turns, and the regret is averaged over 1000 instances.
    }
    \label{fig:comparison}
\end{figure}