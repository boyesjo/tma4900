\section{Relation to machine learning}
\subsection{Reinforcement learning}
Multi-armed bandits can be seen as the simplest case of Markov decision processes (MDPs), the theoretical framework for modern reinforcement learning.
MDPs are a generalisation of Markov chains wherein an agent observes a state from a known space of states, after which it chooses an action from a known set of actions.
For each state-action pair, there are associated transition probabilities to other states with corresponding rewards.
Like in the multi-armed bandit problem, the goal is to maximise the received rewards, and as is the case with bandits, the agent does not know the transition probabilities or the rewards, having instead to learn these through iterated interactions with the environment.

While bandit problems can be considered analytically and solved somewhat optimally, MDPs are generally intractable and far too complex to be solved optimally.
Exempli gratia, the game of go has approximately $10^{170}$ possible states, far too complicated to be solved exactly optimally.
However, with modern machine learning techniques, namely neural networks, it is possible to learn good policies and beat the best human players~\autocite{silver2016}.

Unlike bandits whose performance is important from the start, reinforcement learning is generally tackled by first training the model on a set of data or a simulated environment before deploying it to the real problem at hand.
Consequently, the exploration-exploitation dilemma is not as pressing in reinforcement learning as it is in bandits, but it is still present.
While learning strategies, there is indeed a trade-off between optimising the more promising strategies contra attempting radically different strategies.
Epsilon-greedy strategies can be used, though more advanced strategies with adaptive exploration rates appear to be more effective~\autocite{tokic2011}.


\subsection{Meta-learning}
