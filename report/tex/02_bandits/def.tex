\section{Formal definition}
In the multi-armed bandit problem, a set of $k$ distributions are given, each with an unknown mean $\mu_i$.
Denote the set of distributions by $\mathcal{A}$ and the distributions by $P_i$, $i \in \{1,2,\dots,k\}$.
These distributions have unknown means $\{\mu_1, \mu_2,\dots,\mu_k\}$.
The agent is given a number of turns $T$, known as the (time-) horizon, and at each turn $t$, the agent selects an arm $i_t$ whence it samples independently of previous samples, receiving a reward $r_{i_t} \sim P_{i_t}$.
The goal is to maximise the expected sum of rewards.

To summarise:
\begin{description}
    \item[Known:] Number of arms $k$, number of rounds $T$
    \item[Unknown:] Mean rewards $\{\mu_1, \mu_2,\dots,\mu_k\}$, distributions $\{P_1, P_2,\dots,P_k\}$
    \item[Game:] For $t=1,\dots,T$: select arm $i_t$ and receive reward $r_{i_t} \sim P_{i_t}$ (independently of previous samples)
    \item[Goal:] Maximise $E(\sum_{t=1}^T r_{i_t})$
\end{description}

While the mean rewards always are unknown, some assumptions will be made about the distributions, of which some common ones are listed in \cref{tab:mab_assumptions}.
Naturally, stronger assumptions lead to better algorithms.


\subsection{Performance}
For the analysis of algorithm performance, the regret is used.
At round $T$, the regret is defined as
\begin{equation}
    R(T) = \sum_{t=1}^T \mu^* - \mu_{i_t},
\end{equation}
where $\mu^*$ is the highest, optimal mean, and $\mu_{i_t}$ is the mean of the arm selected at time $t$.
Denoting by $N_i(T)$ the number of times arm $i$ is pulled and $\Delta_i$ the difference between the mean of arm $i$ and the optimal mean, the regret can be rewritten as
\begin{equation}
    R(T) = \sum_{i=1}^k N_i(T) \Delta_i.
\end{equation}
Algorithms are often probabilistic, so often the expected regret is of more interest, given by
\begin{equation}
    E(R(T)) = \sum_{i=1}^k E(N_i(T)) \Delta_i.
\end{equation}
Regret will be used interchangeably with expected regret, with the meaning being clear from the context.
When discussing algorithms in general, the expected regret is of concern, while considering particular simulations, the regret is of interest.
The number of turns $T$ is often referred to as the (time-) horizon and will be assumed to be greater than $k$, such that all arms may be pulled.
While $T$ is assumed given, many algorithms are designed to work independently of it, being known as anytime algorithms.

\begin{table}
    \centering
    \caption{
        Common assumptions made about MAB distributions.
    }
    \label{tab:mab_assumptions}
    \begin{tblr}{
            width=\linewidth,
            colspec={Q[l] Q[r]}
        }
        \toprule
        Assumption                 & Description                                                       \\
        \midrule
        Bernoulli                  & $r \sim \mathcal{B}(\mu)$                                         \\
        Gaussian, unit variance    & $r \sim \mathcal{N}(\mu,1)$                                       \\
        Gaussian, unknown variance & $r \sim \mathcal{N}(\mu,\sigma^2)$                                \\
        Sub-Gaussian               & $\exists C > 0: P(|r| \geq \epsilon) \leq 2\exp(-\epsilon^2/C^2)$ \\
        Bounded value              & $r \in [0,1]$                                                     \\
        \bottomrule
    \end{tblr}
\end{table}