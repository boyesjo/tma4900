\chapter{Stochastic bandits and classical strategies}
\label{chap:bandits}

The multi-armed bandit (MAB) problem is a classic problem in reinforcement learning and probability theory.
It poses a simple yet challenging problem, where a player must sequentially choose between a number of arms (distributions) receiving random rewards, of which the average is unknown, trying to maximise the cumulative reward.
This poses a constant struggle between exploration and exploitation, where exploration is the process of trying out new arms, and exploitation is the process of maximising the reward by pulling what seems to be the best arm.

Although the bandit term was not coined before 1952~\autocite{robbins1952}, its study actually dates back to 1933~\autocite{thompson1933}.
The problem of choosing between two treatments for patients was considered: to what degree should the most successful treatment be used versus testing the other to ensure that the truly best is indeed used?
What strategy is most likely maximise the number of patients treated with the best treatment in the long run?

This chapter and thesis in total will focus on the stochastic bandits, where the rewards are drawn independently from a fixed distribution for each arm.
The notation will mostly follow the textbook~\autocite{lattimore2020}, to which the interested reader is referred for more details on the bandit problem, its variants, applications and history.


\subimport{}{definition}
\subimport{}{variants}
\subimport{}{optimality}
\subimport{}{strats}
